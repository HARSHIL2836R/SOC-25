{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1dc58490",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pymupdf\n",
    "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
    "from langchain_community.vectorstores import FAISS\n",
    "from langchain_huggingface import HuggingFaceEmbeddings\n",
    "from langchain_core.prompts import PromptTemplate\n",
    "from langchain_groq import ChatGroq\n",
    "from langchain.schema import Document\n",
    "from langchain_core.runnables import RunnableParallel, RunnablePassthrough, RunnableLambda\n",
    "from langchain_core.output_parsers import StrOutputParser\n",
    "from langchain.memory import ConversationBufferMemory\n",
    "import os\n",
    "import re\n",
    "\n",
    "from dotenv import load_dotenv\n",
    "\n",
    "load_dotenv()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5606e6a1",
   "metadata": {},
   "source": [
    "# Research Paper Analysis Agent\n",
    "\n",
    "This notebook implements an LLM-based research agent that can analyze and answer questions about research papers. The agent extracts content from PDF research papers, creates a searchable knowledge base, and provides contextual answers with conversation memory."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f2c30e66",
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_paper_sections(pdf_path):\n",
    "    \"\"\"Extract sections from a research paper PDF\"\"\"\n",
    "    doc = pymupdf.open(pdf_path)\n",
    "    \n",
    "    # Common research paper section patterns\n",
    "    section_patterns = [\n",
    "        r'^(abstract|introduction|related work|methodology|method|approach|implementation|results|discussion|conclusion|references|acknowledgments)',\n",
    "        r'^\\d+\\.?\\s+(abstract|introduction|related work|methodology|method|approach|implementation|results|discussion|conclusion|references|acknowledgments)',\n",
    "        r'^\\d+\\.\\d+\\.?\\s+.*',  # Subsections like 2.1, 3.2\n",
    "    ]\n",
    "    \n",
    "    sections = {}\n",
    "    current_section = \"content\"\n",
    "    section_content = []\n",
    "    \n",
    "    print(f\"Extracting content from: {pdf_path}\")\n",
    "    print(f\"Total pages: {doc.page_count}\")\n",
    "    \n",
    "    for page_num in range(doc.page_count):\n",
    "        page = doc[page_num]\n",
    "        text = page.get_text()\n",
    "        lines = text.split('\\n')\n",
    "        \n",
    "        for line in lines:\n",
    "            line = line.strip()\n",
    "            if not line:\n",
    "                continue\n",
    "                \n",
    "            # Check if this line is a section header\n",
    "            is_section_header = False\n",
    "            for pattern in section_patterns:\n",
    "                if re.match(pattern, line.lower()):\n",
    "                    # Save previous section\n",
    "                    if section_content:\n",
    "                        sections[current_section] = '\\n'.join(section_content)\n",
    "                    \n",
    "                    # Start new section\n",
    "                    current_section = line.lower()\n",
    "                    section_content = []\n",
    "                    is_section_header = True\n",
    "                    break\n",
    "            \n",
    "            if not is_section_header:\n",
    "                section_content.append(line)\n",
    "    \n",
    "    # Save the last section\n",
    "    if section_content:\n",
    "        sections[current_section] = '\\n'.join(section_content)\n",
    "    \n",
    "    doc.close()\n",
    "    \n",
    "    print(f\"Extracted sections: {list(sections.keys())}\")\n",
    "    return sections\n",
    "\n",
    "def extract_paper_metadata(pdf_path):\n",
    "    \"\"\"Extract basic metadata from research paper\"\"\"\n",
    "    doc = pymupdf.open(pdf_path)\n",
    "    \n",
    "    # Get document metadata\n",
    "    metadata = doc.metadata\n",
    "    \n",
    "    # Extract first page text to get title and authors\n",
    "    first_page = doc[0].get_text()\n",
    "    lines = first_page.split('\\n')\n",
    "    \n",
    "    # Simple heuristic to find title (usually first few lines with substantial text)\n",
    "    title = \"Unknown Title\"\n",
    "    authors = \"Unknown Authors\"\n",
    "    \n",
    "    substantial_lines = [line.strip() for line in lines if len(line.strip()) > 10]\n",
    "    if substantial_lines:\n",
    "        title = substantial_lines[0]\n",
    "        if len(substantial_lines) > 1:\n",
    "            # Look for author pattern (names, emails, affiliations)\n",
    "            for line in substantial_lines[1:4]:\n",
    "                if any(indicator in line.lower() for indicator in ['@', 'university', 'institute', 'college']):\n",
    "                    authors = line\n",
    "                    break\n",
    "    \n",
    "    doc.close()\n",
    "    \n",
    "    return {\n",
    "        'title': title,\n",
    "        'authors': authors,\n",
    "        'filename': os.path.basename(pdf_path),\n",
    "        'total_pages': doc.page_count if 'doc' in locals() else 0\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "16dfbcd3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# URL Paper Retrieval Agent using LangChain\n",
    "import requests\n",
    "from urllib.parse import urlparse\n",
    "from langchain_community.document_loaders import PyPDFLoader, WebBaseLoader\n",
    "from langchain_community.document_loaders import UnstructuredURLLoader\n",
    "import tempfile\n",
    "import urllib.request\n",
    "\n",
    "def download_paper_from_url(url, save_path=None):\n",
    "    \"\"\"Download paper from URL and save locally\"\"\"\n",
    "    try:\n",
    "        # Parse URL to get filename\n",
    "        parsed_url = urlparse(url)\n",
    "        filename = parsed_url.path.split('/')[-1]\n",
    "        \n",
    "        # If no filename extension, assume PDF\n",
    "        if not filename.endswith('.pdf'):\n",
    "            filename = filename + '.pdf' if filename else 'downloaded_paper.pdf'\n",
    "            \n",
    "        # Set save path\n",
    "        if save_path is None:\n",
    "            save_path = filename\n",
    "            \n",
    "        print(f\"Downloading paper from: {url}\")\n",
    "        print(f\"Saving to: {save_path}\")\n",
    "        \n",
    "        # Download the file\n",
    "        urllib.request.urlretrieve(url, save_path)\n",
    "        \n",
    "        print(f\"Successfully downloaded: {save_path}\")\n",
    "        return save_path\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"Error downloading paper: {e}\")\n",
    "        return None\n",
    "\n",
    "def extract_paper_from_url(url, use_temp_file=True):\n",
    "    \"\"\"Extract paper content directly from URL using LangChain loaders\"\"\"\n",
    "    try:\n",
    "        print(f\"Extracting paper content from URL: {url}\")\n",
    "        \n",
    "        # Check if URL points to a PDF\n",
    "        if url.lower().endswith('.pdf') or 'pdf' in url.lower():\n",
    "            if use_temp_file:\n",
    "                # Download to temporary file first\n",
    "                with tempfile.NamedTemporaryFile(delete=False, suffix='.pdf') as tmp_file:\n",
    "                    urllib.request.urlretrieve(url, tmp_file.name)\n",
    "                    \n",
    "                    # Use PyPDFLoader for better PDF handling\n",
    "                    loader = PyPDFLoader(tmp_file.name)\n",
    "                    documents = loader.load()\n",
    "                    \n",
    "                    # Clean up temp file\n",
    "                    import os\n",
    "                    os.unlink(tmp_file.name)\n",
    "                    \n",
    "            else:\n",
    "                # Try direct URL loading (may not work for all PDFs)\n",
    "                loader = PyPDFLoader(url)\n",
    "                documents = loader.load()\n",
    "                \n",
    "        else:\n",
    "            # For web pages, use WebBaseLoader\n",
    "            loader = WebBaseLoader(url)\n",
    "            documents = loader.load()\n",
    "        \n",
    "        print(f\"Successfully extracted {len(documents)} pages/sections from URL\")\n",
    "        \n",
    "        # Combine all document content\n",
    "        full_text = \"\\n\\n\".join([doc.page_content for doc in documents])\n",
    "        \n",
    "        # Extract basic metadata\n",
    "        metadata = {\n",
    "            'source_url': url,\n",
    "            'total_pages': len(documents),\n",
    "            'content_length': len(full_text)\n",
    "        }\n",
    "        \n",
    "        return full_text, metadata, documents\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"Error extracting paper from URL: {e}\")\n",
    "        return None, None, None\n",
    "\n",
    "def process_arxiv_url(arxiv_url):\n",
    "    \"\"\"Convert arXiv abstract URL to PDF URL\"\"\"\n",
    "    if 'arxiv.org/abs/' in arxiv_url:\n",
    "        # Convert abstract URL to PDF URL\n",
    "        paper_id = arxiv_url.split('/abs/')[-1]\n",
    "        pdf_url = f\"https://arxiv.org/pdf/{paper_id}.pdf\"\n",
    "        return pdf_url\n",
    "    return arxiv_url\n",
    "\n",
    "def create_url_paper_agent(url):\n",
    "    \"\"\"Create a complete paper analysis setup from URL\"\"\"\n",
    "    global paper_sections, paper_metadata, documents, vector_store\n",
    "    \n",
    "    try:\n",
    "        # Handle arXiv URLs\n",
    "        if 'arxiv.org' in url:\n",
    "            url = process_arxiv_url(url)\n",
    "            print(f\"Converted to PDF URL: {url}\")\n",
    "        \n",
    "        # Extract paper content from URL\n",
    "        full_text, url_metadata, raw_documents = extract_paper_from_url(url)\n",
    "        \n",
    "        if full_text is None:\n",
    "            print(\"Failed to extract paper content from URL\")\n",
    "            return False\n",
    "        \n",
    "        # Simple section extraction for URL-based papers\n",
    "        # This is a simplified approach - you might want to enhance this\n",
    "        sections = extract_sections_from_text(full_text)\n",
    "        \n",
    "        # Create paper metadata\n",
    "        paper_metadata = {\n",
    "            'title': extract_title_from_text(full_text),\n",
    "            'authors': extract_authors_from_text(full_text),\n",
    "            'source_url': url,\n",
    "            'filename': url.split('/')[-1],\n",
    "            'total_pages': url_metadata['total_pages'],\n",
    "            'content_length': url_metadata['content_length']\n",
    "        }\n",
    "        \n",
    "        paper_sections = sections\n",
    "        \n",
    "        print(\"\\n=== URL Paper Metadata ===\")\n",
    "        for key, value in paper_metadata.items():\n",
    "            print(f\"{key}: {value}\")\n",
    "            \n",
    "        print(f\"\\n=== Sections Found ===\")\n",
    "        for section, content in paper_sections.items():\n",
    "            print(f\"- {section}: {len(content)} characters\")\n",
    "            \n",
    "        return True\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"Error processing URL paper: {e}\")\n",
    "        return False\n",
    "\n",
    "def extract_sections_from_text(text):\n",
    "    \"\"\"Extract sections from full text (simplified approach)\"\"\"\n",
    "    sections = {}\n",
    "    \n",
    "    # Split by common section headers\n",
    "    section_patterns = [\n",
    "        r'\\n\\s*(abstract|introduction|related work|methodology|method|approach|implementation|results|discussion|conclusion|references|acknowledgments)\\s*\\n',\n",
    "        r'\\n\\s*\\d+\\.?\\s*(abstract|introduction|related work|methodology|method|approach|implementation|results|discussion|conclusion|references|acknowledgments)\\s*\\n',\n",
    "    ]\n",
    "    \n",
    "    # For now, return the full text as 'content' section\n",
    "    # You can enhance this to better parse sections\n",
    "    sections['full_content'] = text\n",
    "    \n",
    "    # Try to find abstract\n",
    "    import re\n",
    "    abstract_match = re.search(r'abstract\\s*[:\\-\\n]\\s*(.*?)(?=\\n\\s*(?:introduction|keywords|\\d+\\.|$))', text, re.IGNORECASE | re.DOTALL)\n",
    "    if abstract_match:\n",
    "        sections['abstract'] = abstract_match.group(1).strip()\n",
    "    \n",
    "    return sections\n",
    "\n",
    "def extract_title_from_text(text):\n",
    "    \"\"\"Extract title from paper text (simple heuristic)\"\"\"\n",
    "    lines = text.split('\\n')\n",
    "    # Look for the first substantial line as title\n",
    "    for line in lines[:10]:\n",
    "        line = line.strip()\n",
    "        if len(line) > 20 and not line.lower().startswith(('abstract', 'keywords', 'introduction')):\n",
    "            return line\n",
    "    return \"Unknown Title from URL\"\n",
    "\n",
    "def extract_authors_from_text(text):\n",
    "    \"\"\"Extract authors from paper text (simple heuristic)\"\"\"\n",
    "    lines = text.split('\\n')\n",
    "    # Look for author patterns in first few lines\n",
    "    for line in lines[1:8]:\n",
    "        line = line.strip()\n",
    "        if any(indicator in line.lower() for indicator in ['@', 'university', 'institute', 'college', 'department']):\n",
    "            return line\n",
    "    return \"Unknown Authors from URL\"\n",
    "\n",
    "# Example usage functions\n",
    "def load_paper_from_arxiv(arxiv_id):\n",
    "    \"\"\"Load paper directly from arXiv ID\"\"\"\n",
    "    arxiv_url = f\"https://arxiv.org/abs/{arxiv_id}\"\n",
    "    return create_url_paper_agent(arxiv_url)\n",
    "\n",
    "def load_paper_from_pdf_url(pdf_url):\n",
    "    \"\"\"Load paper from direct PDF URL\"\"\"\n",
    "    return create_url_paper_agent(pdf_url)\n",
    "\n",
    "print(\"URL Paper Retrieval Agent loaded successfully!\")\n",
    "print(\"Usage:\")\n",
    "print(\"- create_url_paper_agent('https://arxiv.org/abs/2301.xxxxx')\")\n",
    "print(\"- load_paper_from_arxiv('2301.xxxxx')\")\n",
    "print(\"- load_paper_from_pdf_url('https://example.com/paper.pdf')\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f8ae396a",
   "metadata": {},
   "source": [
    "## URL Paper Retrieval Agent\n",
    "\n",
    "This section implements functionality to retrieve and process research papers directly from URLs using LangChain document loaders. \n",
    "\n",
    "**Supported Sources:**\n",
    "- arXiv papers (both abstract and direct PDF URLs)\n",
    "- Direct PDF URLs from any website\n",
    "- Web pages containing research content\n",
    "\n",
    "**Key Features:**\n",
    "- Automatic arXiv URL conversion (abstract â†’ PDF)\n",
    "- Temporary file handling for secure downloads\n",
    "- LangChain PyPDFLoader integration\n",
    "- Automatic metadata extraction from URL content\n",
    "- Seamless integration with the existing RAG pipeline\n",
    "\n",
    "**Usage Examples:**\n",
    "```python\n",
    "# From arXiv (using paper ID)\n",
    "load_paper_from_arxiv(\"2301.08727\")\n",
    "\n",
    "# From arXiv (using full URL)\n",
    "create_url_paper_agent(\"https://arxiv.org/abs/2301.08727\")\n",
    "\n",
    "# From direct PDF URL\n",
    "create_url_paper_agent(\"https://example.com/research_paper.pdf\")\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "536444eb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Configure your research paper source here\n",
    "PAPER_SOURCE = \"doc.pdf\"  # Can be local file path or URL\n",
    "\n",
    "# Check if source is URL or local file\n",
    "if PAPER_SOURCE.startswith(('http://', 'https://')):\n",
    "    print(\"Loading paper from URL...\")\n",
    "    success = create_url_paper_agent(PAPER_SOURCE)\n",
    "    if not success:\n",
    "        print(\"Failed to load paper from URL. Please check the URL and try again.\")\n",
    "        # You might want to set some default values here\n",
    "        paper_sections = {\"error\": \"Failed to load from URL\"}\n",
    "        paper_metadata = {\"title\": \"Error\", \"authors\": \"Unknown\", \"filename\": \"error\"}\n",
    "else:\n",
    "    print(\"Loading paper from local file...\")\n",
    "    # Extract paper content and metadata from local file\n",
    "    paper_sections = extract_paper_sections(PAPER_SOURCE)\n",
    "    paper_metadata = extract_paper_metadata(PAPER_SOURCE)\n",
    "\n",
    "print(\"\\n=== Paper Metadata ===\")\n",
    "for key, value in paper_metadata.items():\n",
    "    print(f\"{key}: {value}\")\n",
    "\n",
    "print(f\"\\n=== Sections Found ===\")\n",
    "for section, content in paper_sections.items():\n",
    "    print(f\"- {section}: {len(content)} characters\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "922abd80",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Examples of URL Paper Loading\n",
    "# Uncomment and modify one of these examples to load papers from URLs:\n",
    "\n",
    "# Example 1: Load from arXiv using paper ID\n",
    "# load_paper_from_arxiv(\"2301.08727\")  # Replace with actual arXiv ID\n",
    "\n",
    "# Example 2: Load from arXiv using full URL\n",
    "# create_url_paper_agent(\"https://arxiv.org/abs/2301.08727\")  # Replace with actual arXiv URL\n",
    "\n",
    "# Example 3: Load from direct PDF URL\n",
    "# create_url_paper_agent(\"https://example.com/paper.pdf\")  # Replace with actual PDF URL\n",
    "\n",
    "# Example 4: Download and save paper locally first\n",
    "# pdf_url = \"https://arxiv.org/pdf/2301.08727.pdf\"\n",
    "# local_path = download_paper_from_url(pdf_url, \"downloaded_paper.pdf\")\n",
    "# if local_path:\n",
    "#     paper_sections = extract_paper_sections(local_path)\n",
    "#     paper_metadata = extract_paper_metadata(local_path)\n",
    "\n",
    "print(\"URL examples are ready to use!\")\n",
    "print(\"Uncomment and modify the examples above to load papers from URLs.\")\n",
    "print(\"Current source:\", PAPER_SOURCE)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5b96440a",
   "metadata": {},
   "source": [
    "# Initialize LLM and Embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "32f31be3",
   "metadata": {},
   "outputs": [],
   "source": [
    "llm = ChatGroq(\n",
    "    model=\"llama-3.1-8b-instant\",\n",
    "    temperature=0.1,\n",
    "    max_tokens=1000\n",
    ")\n",
    "\n",
    "embeddings = HuggingFaceEmbeddings(\n",
    "    model_name=\"sentence-transformers/all-MiniLM-L6-v2\",\n",
    "    encode_kwargs={\"normalize_embeddings\": True}\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fe63b768",
   "metadata": {},
   "source": [
    "# Create Document Chunks and Vector Store"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "073ce77e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Text splitter optimized for research papers\n",
    "text_splitter = RecursiveCharacterTextSplitter(\n",
    "    chunk_size=1000,  # Larger chunks for research content\n",
    "    chunk_overlap=200,  # More overlap to preserve context\n",
    "    length_function=len,\n",
    "    separators=[\"\\n\\n\", \"\\n\", \". \", \"! \", \"? \", \", \", \" \", \"\"]\n",
    ")\n",
    "\n",
    "# Create documents from paper sections\n",
    "documents = []\n",
    "\n",
    "for section_name, section_content in paper_sections.items():\n",
    "    if len(section_content.strip()) < 50:  # Skip very short sections\n",
    "        continue\n",
    "        \n",
    "    chunks = text_splitter.split_text(section_content)\n",
    "    \n",
    "    for i, chunk in enumerate(chunks):\n",
    "        doc = Document(\n",
    "            page_content=chunk,\n",
    "            metadata={\n",
    "                \"source\": paper_metadata['filename'],\n",
    "                \"paper_title\": paper_metadata['title'],\n",
    "                \"authors\": paper_metadata['authors'],\n",
    "                \"section\": section_name,\n",
    "                \"chunk_id\": i,\n",
    "                \"chunk_size\": len(chunk)\n",
    "            }\n",
    "        )\n",
    "        documents.append(doc)\n",
    "\n",
    "print(f\"Created {len(documents)} document chunks from the research paper\")\n",
    "print(f\"Average chunk size: {sum(len(doc.page_content) for doc in documents) // len(documents)} characters\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6623637f",
   "metadata": {},
   "source": [
    "### Vector Store Creation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c0ab7261",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create vector store from documents\n",
    "vector_store = FAISS.from_documents(\n",
    "    documents=documents,\n",
    "    embedding=embeddings\n",
    ")\n",
    "\n",
    "print(\"Vector store created successfully!\")\n",
    "print(f\"Total documents indexed: {len(documents)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "542634b5",
   "metadata": {},
   "source": [
    "# Research Agent RAG Pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "17f9286d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create retriever with research-focused search\n",
    "retriever = vector_store.as_retriever(\n",
    "    search_type=\"similarity\",\n",
    "    search_kwargs={\"k\": 6}  # Retrieve top 6 most relevant chunks for research\n",
    ")\n",
    "\n",
    "# Enhanced prompt template for research paper analysis\n",
    "research_prompt = PromptTemplate(\n",
    "    template=\"\"\"You are an expert research assistant specializing in academic paper analysis.\n",
    "\n",
    "PAPER INFORMATION:\n",
    "Title: {paper_title}\n",
    "Authors: {authors}\n",
    "\n",
    "INSTRUCTIONS:\n",
    "- Analyze the provided research paper content to answer questions accurately\n",
    "- Reference specific sections, methodologies, results, and findings when relevant\n",
    "- Maintain academic rigor and cite evidence from the paper\n",
    "- Use the conversation history to provide coherent, contextual responses\n",
    "- If the question requires information not in the provided context, clearly state the limitations\n",
    "- For technical questions, explain concepts clearly while maintaining accuracy\n",
    "\n",
    "CONVERSATION HISTORY:\n",
    "{chat_history}\n",
    "\n",
    "RELEVANT PAPER CONTENT:\n",
    "{context}\n",
    "\n",
    "RESEARCH QUESTION: {question}\n",
    "\n",
    "DETAILED ANALYSIS:\"\"\",\n",
    "    input_variables=['context', 'question', 'chat_history', 'paper_title', 'authors']\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d6ebb429",
   "metadata": {},
   "outputs": [],
   "source": [
    "def format_research_docs(retrieved_docs):\n",
    "    \"\"\"Format retrieved documents with section information\"\"\"\n",
    "    formatted_content = []\n",
    "    for doc in retrieved_docs:\n",
    "        section = doc.metadata.get('section', 'Unknown Section')\n",
    "        content = doc.page_content\n",
    "        formatted_content.append(f\"[Section: {section}]\\n{content}\")\n",
    "    \n",
    "    return \"\\n\\n\" + \"=\"*50 + \"\\n\\n\".join(formatted_content)\n",
    "\n",
    "def extract_question(inputs):\n",
    "    return inputs.get('question', '')\n",
    "\n",
    "def extract_chat_history(inputs):\n",
    "    return inputs.get('chat_history', '')\n",
    "\n",
    "def get_paper_metadata(inputs):\n",
    "    return {\n",
    "        'paper_title': paper_metadata['title'],\n",
    "        'authors': paper_metadata['authors']\n",
    "    }\n",
    "\n",
    "# Parallel processing chain for research agent\n",
    "research_parallel_chain = RunnableParallel({\n",
    "    'context': RunnableLambda(extract_question) | retriever | RunnableLambda(format_research_docs),\n",
    "    'question': RunnableLambda(extract_question),\n",
    "    'chat_history': RunnableLambda(extract_chat_history),\n",
    "    'paper_title': RunnableLambda(lambda x: paper_metadata['title']),\n",
    "    'authors': RunnableLambda(lambda x: paper_metadata['authors'])\n",
    "})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "12f61fa2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Complete research agent chain\n",
    "parser = StrOutputParser()\n",
    "research_chain = research_parallel_chain | research_prompt | llm | parser"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cc6362be",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Research agent with conversation memory\n",
    "research_memory = ConversationBufferMemory(\n",
    "    memory_key=\"chat_history\",\n",
    "    input_key=\"question\",\n",
    "    output_key=\"answer\",\n",
    "    return_messages=False\n",
    ")\n",
    "\n",
    "def display_research_history():\n",
    "    \"\"\"Display the research conversation history\"\"\"\n",
    "    chat_history = research_memory.load_memory_variables({})[\"chat_history\"]\n",
    "    if chat_history and chat_history.strip():\n",
    "        print(\"=== RESEARCH CONVERSATION HISTORY ===\")\n",
    "        print(chat_history)\n",
    "        print(\"=== END HISTORY ===\")\n",
    "    else:\n",
    "        print(\"No research conversation history yet.\")\n",
    "\n",
    "def clear_research_history():\n",
    "    \"\"\"Clear all research conversation history\"\"\"\n",
    "    research_memory.clear()\n",
    "    print(\"Research conversation history cleared.\")\n",
    "\n",
    "def research_qa_with_memory(question):\n",
    "    \"\"\"Research QA with persistent memory\"\"\"\n",
    "    # Get chat history from memory\n",
    "    chat_history = research_memory.load_memory_variables({})[\"chat_history\"]\n",
    "    \n",
    "    # Ensure chat_history is a string\n",
    "    if isinstance(chat_history, list):\n",
    "        chat_history = \"\\n\".join(str(x) for x in chat_history)\n",
    "    elif chat_history is None:\n",
    "        chat_history = \"\"\n",
    "    \n",
    "    # Run the research chain\n",
    "    result = research_chain.invoke({\n",
    "        \"question\": question,\n",
    "        \"chat_history\": chat_history\n",
    "    })\n",
    "    \n",
    "    # Extract answer if it's a dict\n",
    "    if isinstance(result, dict) and \"answer\" in result:\n",
    "        answer = result[\"answer\"]\n",
    "    else:\n",
    "        answer = result\n",
    "    \n",
    "    # Save to memory\n",
    "    research_memory.save_context({\"question\": question}, {\"answer\": answer})\n",
    "    \n",
    "    return answer\n",
    "\n",
    "def interactive_research_chat():\n",
    "    \"\"\"Interactive research chat session\"\"\"\n",
    "    print(\"=== Research Paper Analysis Agent ===\")\n",
    "    print(f\"Analyzing: {paper_metadata['title']}\")\n",
    "    print(f\"Authors: {paper_metadata['authors']}\")\n",
    "    print(\"\\nCommands:\")\n",
    "    print(\"- Type your research questions normally\")\n",
    "    print(\"- 'quit' to exit\")\n",
    "    print(\"- 'history' to see conversation history\")\n",
    "    print(\"- 'clear' to clear history\")\n",
    "    print(\"- 'sections' to see available paper sections\")\n",
    "    print(\"- 'metadata' to see paper information\")\n",
    "    \n",
    "    while True:\n",
    "        user_question = input(\"\\nResearcher: \")\n",
    "        \n",
    "        if user_question.lower() == 'quit':\n",
    "            break\n",
    "        elif user_question.lower() == 'history':\n",
    "            display_research_history()\n",
    "            continue\n",
    "        elif user_question.lower() == 'clear':\n",
    "            clear_research_history()\n",
    "            continue\n",
    "        elif user_question.lower() == 'sections':\n",
    "            print(\"Available paper sections:\")\n",
    "            for section in paper_sections.keys():\n",
    "                print(f\"- {section}\")\n",
    "            continue\n",
    "        elif user_question.lower() == 'metadata':\n",
    "            print(\"Paper Information:\")\n",
    "            for key, value in paper_metadata.items():\n",
    "                print(f\"- {key}: {value}\")\n",
    "            continue\n",
    "        \n",
    "        try:\n",
    "            response = research_qa_with_memory(user_question)\n",
    "            print(f\"\\nAgent: {response}\\n\")\n",
    "        except Exception as e:\n",
    "            print(f\"Error: {e}\")\n",
    "\n",
    "# Function for single question testing\n",
    "def ask_research_question(question):\n",
    "    \"\"\"Ask a single research question\"\"\"\n",
    "    response = research_qa_with_memory(question)\n",
    "    print(f\"Question: {question}\")\n",
    "    print(f\"Answer: {response}\")\n",
    "    return response"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "78bd2229",
   "metadata": {},
   "source": [
    "# Test the Research Agent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b44d907d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test with some sample research questions\n",
    "print(\"=== Testing Research Agent ===\\n\")\n",
    "\n",
    "# Test questions - modify these based on your research paper\n",
    "test_questions = [\n",
    "    \"What is the main contribution of this paper?\",\n",
    "    \"What methodology does this paper use?\",\n",
    "    \"What are the key findings or results?\",\n",
    "    \"What are the limitations mentioned in the paper?\"\n",
    "]\n",
    "\n",
    "for question in test_questions:\n",
    "    print(f\"Q: {question}\")\n",
    "    try:\n",
    "        answer = ask_research_question(question)\n",
    "        print(f\"A: {answer}\\n\")\n",
    "        print(\"-\" * 80)\n",
    "    except Exception as e:\n",
    "        print(f\"Error: {e}\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fb19a57f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Start interactive research session\n",
    "# Uncomment the line below to start the interactive chat\n",
    "interactive_research_chat()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a0b237b5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Additional utility functions for research analysis\n",
    "\n",
    "def get_section_summary(section_name):\n",
    "    \"\"\"Get a summary of a specific section\"\"\"\n",
    "    if section_name.lower() in paper_sections:\n",
    "        content = paper_sections[section_name.lower()]\n",
    "        question = f\"Please provide a concise summary of the {section_name} section\"\n",
    "        \n",
    "        # Create a temporary retriever for this specific section\n",
    "        section_docs = [doc for doc in documents if doc.metadata.get('section', '').lower() == section_name.lower()]\n",
    "        if section_docs:\n",
    "            # Use the research agent to summarize\n",
    "            response = research_qa_with_memory(question)\n",
    "            return response\n",
    "        else:\n",
    "            return f\"Section '{section_name}' not found or has no content.\"\n",
    "    else:\n",
    "        available_sections = list(paper_sections.keys())\n",
    "        return f\"Section '{section_name}' not found. Available sections: {available_sections}\"\n",
    "\n",
    "def compare_with_related_work():\n",
    "    \"\"\"Generate questions about related work and comparisons\"\"\"\n",
    "    related_work_questions = [\n",
    "        \"What related work does this paper cite?\",\n",
    "        \"How does this work differ from previous approaches?\",\n",
    "        \"What gaps in existing research does this paper address?\"\n",
    "    ]\n",
    "    \n",
    "    print(\"=== Related Work Analysis ===\")\n",
    "    for question in related_work_questions:\n",
    "        try:\n",
    "            answer = research_qa_with_memory(question)\n",
    "            print(f\"\\nQ: {question}\")\n",
    "            print(f\"A: {answer}\")\n",
    "            print(\"-\" * 60)\n",
    "        except Exception as e:\n",
    "            print(f\"Error analyzing related work: {e}\")\n",
    "\n",
    "def analyze_methodology():\n",
    "    \"\"\"Analyze the paper's methodology\"\"\"\n",
    "    method_questions = [\n",
    "        \"What is the experimental setup?\",\n",
    "        \"What datasets are used?\",\n",
    "        \"What evaluation metrics are employed?\",\n",
    "        \"What are the implementation details?\"\n",
    "    ]\n",
    "    \n",
    "    print(\"=== Methodology Analysis ===\")\n",
    "    for question in method_questions:\n",
    "        try:\n",
    "            answer = research_qa_with_memory(question)\n",
    "            print(f\"\\nQ: {question}\")\n",
    "            print(f\"A: {answer}\")\n",
    "            print(\"-\" * 60)\n",
    "        except Exception as e:\n",
    "            print(f\"Error analyzing methodology: {e}\")\n",
    "\n",
    "# Example usage:\n",
    "# print(get_section_summary(\"introduction\"))\n",
    "# compare_with_related_work()\n",
    "# analyze_methodology()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "049ce0a0",
   "metadata": {},
   "source": [
    "# ðŸš€ Streamlit Dashboard\n",
    "\n",
    "A complete web-based dashboard has been created as `research_dashboard.py` that provides:\n",
    "\n",
    "## Features:\n",
    "- **ðŸ“‚ Paper Upload**: Upload PDF files or enter URLs (including arXiv)\n",
    "- **ðŸ’¾ Automatic Saving**: Papers are saved to a `papers/` folder for future use\n",
    "- **ðŸ“š Paper Library**: Browse and reload previously saved papers\n",
    "- **ðŸ’¬ Interactive Chat**: Natural language conversation with your research papers\n",
    "- **ðŸ“‹ Paper Information**: Display metadata and sections\n",
    "- **ðŸš€ Quick Actions**: Pre-built queries for common research questions\n",
    "- **ðŸ“‘ Section Explorer**: Browse paper sections interactively\n",
    "- **ðŸ—‚ï¸ Paper Management**: Delete unwanted papers to save space\n",
    "\n",
    "## How to Run:\n",
    "\n",
    "1. **Install Streamlit** (if not already installed):\n",
    "   ```bash\n",
    "   pip install streamlit\n",
    "   ```\n",
    "\n",
    "2. **Run the dashboard**:\n",
    "   ```bash\n",
    "   streamlit run research_dashboard.py\n",
    "   ```\n",
    "\n",
    "3. **Open your browser** and navigate to the displayed URL (usually `http://localhost:8501`)\n",
    "\n",
    "## Dashboard Components:\n",
    "\n",
    "### Sidebar:\n",
    "- Paper upload/URL input\n",
    "- Paper metadata display\n",
    "- Saved papers library with reload functionality\n",
    "- Paper management (view, load, delete)\n",
    "- Clear paper option\n",
    "\n",
    "### Main Area:\n",
    "- Chat interface with conversation history\n",
    "- Quick action buttons (Summarize, Methodology, Key Findings)\n",
    "- Paper sections explorer\n",
    "\n",
    "### File Organization:\n",
    "- **`papers/` folder**: All downloaded and uploaded PDFs are saved here\n",
    "- **Persistent storage**: Papers remain available between sessions\n",
    "- **Smart naming**: arXiv papers are named with their ID for easy identification\n",
    "\n",
    "### Supported Sources:\n",
    "- **Local PDF files** - Upload directly (saved to papers folder)\n",
    "- **arXiv papers** - Enter abstract or PDF URLs (auto-downloaded and saved)\n",
    "- **Direct PDF URLs** - Any publicly accessible PDF (downloaded and saved)\n",
    "\n",
    "## New File Management Features:\n",
    "\n",
    "### Automatic Paper Saving:\n",
    "- **Uploaded PDFs**: Saved to `papers/` folder with original filename\n",
    "- **Downloaded papers**: Saved with descriptive names (e.g., `arxiv_1706.03762.pdf`)\n",
    "- **Persistent storage**: Papers remain available for future sessions\n",
    "\n",
    "### Paper Library:\n",
    "- **Browse saved papers**: View all papers in the sidebar\n",
    "- **Quick reload**: Select and load any previously saved paper\n",
    "- **File management**: Delete papers you no longer need\n",
    "- **Storage info**: See file sizes and manage disk space\n",
    "\n",
    "The dashboard integrates all the functionality from this notebook into a user-friendly web interface with persistent paper storage!"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
