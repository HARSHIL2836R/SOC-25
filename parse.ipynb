{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "e3169257",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pymupdf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "23448601",
   "metadata": {},
   "outputs": [],
   "source": [
    "doc = pymupdf.open(\"doc.pdf\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "08b62de6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'format': 'PDF 1.7', 'title': 'Automating Ground Control Point Detection in Drone Imagery: From Computer Vision to Deep Learning', 'author': 'Gonzalo Muradás Odriozola, Klaas Pauly, Samuel Oswald and Dries Raymaekers', 'subject': 'Drone-based photogrammetry typically requires the task of georeferencing aerial images by detecting the center of Ground Control Points (GCPs) placed in the field. Since this is a very labor-intensive task, it could benefit greatly from automation. In this study, we explore the extent to which traditional computer vision approaches can be generalized to deal with variability in real-world drone data sets and focus on training different residual neural networks (ResNet) to improve generalization. The models were trained to detect single keypoints of fixed-sized image tiles with a historic collection of drone-based Red–Green–Blue (RGB) images with black and white GCP markers in which the center was manually labeled by experienced photogrammetry operators. Different depths of ResNets and various hyperparameters (learning rate, batch size) were tested. The best results reached sub-pixel accuracy with a mean absolute error of 0.586. The paper demonstrates that this approach to drone-based mapping is a promising and effective way to reduce the human workload required for georeferencing aerial images.', 'keywords': 'drones; photogrammetry; ground control points; GCPs; RGB; computer vision; deep learning; ResNet; CNN', 'creator': 'LaTeX with hyperref', 'producer': 'pdfTeX-1.40.25', 'creationDate': \"D:20240226171410+08'00'\", 'modDate': \"D:20240226102016+01'00'\", 'trapped': '', 'encryption': None}\n"
     ]
    }
   ],
   "source": [
    "print(doc.metadata)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "fe772219",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[1, 'Introduction', 1], [1, 'Materials and Methods', 4], [2, 'Existing Data Set', 4], [2, 'Pre-Processing', 6], [2, 'Data Augmentation', 6], [2, 'Computer Vision Approach', 7], [2, 'Deep Learning Models', 8], [2, 'Experimental Setup', 10], [1, 'Results', 11], [2, 'Computer Vision Approach', 11], [2, 'Deep Learning Approach: Influence of Tile Size', 13], [3, 'Influence with Training', 13], [3, 'Influence with Testing', 13], [2, 'Deep Learning Approach: Main Results', 15], [2, 'Deep Learning Approach: Individual Model Analysis', 17], [2, 'Deep Learning: Training Time', 21], [2, 'Deep Learning: Prediction Time', 21], [1, 'Discussion', 21], [1, 'Additional Content', 23], [1, 'Conclusions', 25], [1, 'References', 26]]\n"
     ]
    }
   ],
   "source": [
    "print(doc.get_toc())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "85138f7f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Citation: Muradás Odriozola, G.;\\nPauly, K.; Samuel, O.; Raymaekers, D.\\nAutomating Ground Control Point\\nDetection in Drone Imagery: From\\nComputer Vision to Deep Learning.\\nRemote Sens. 2024, 16, 794. https://\\ndoi.org/10.3390/rs16050794\\nAcademic Editors: Lidia M. Ortega\\nAlvarado and María I. Ramos Galan\\nReceived: 24 November 2023\\nRevised: 14 February 2024\\nAccepted: 19 February 2024\\nPublished: 24 February 2024\\nCopyright: © 2024 by the authors.\\nLicensee MDPI, Basel, Switzerland.\\nThis article is an open access article\\ndistributed\\nunder\\nthe\\nterms\\nand\\nconditions of the Creative Commons\\nAttribution (CC BY) license (https://\\ncreativecommons.org/licenses/by/\\n4.0/).\\nremote sensing \\nArticle\\nAutomating Ground Control Point Detection in Drone Imagery:\\nFrom Computer Vision to Deep Learning\\nGonzalo Muradás Odriozola 1,2\\n, Klaas Pauly 3,*\\n, Samuel Oswald 3 and Dries Raymaekers 3\\n1\\nImage and Speech Processing (PSI), Department of Electrical Engineering (ESAT), KU Leuven,\\nB-3000 Leuven, Belgium; gonzalo.muradasodriozola@kuleuven.be\\n2\\nLaboratory for Experimental Psychology, Faculty of Psychology and Educational Sciences, KU Leuven,\\nB-3000 Leuven, Belgium\\n3\\nRemote Sensing Unit, Flemish Institute for Technological Research (VITO), Boeretang 200,\\nB-2400 Mol, Belgium; sam.oswald@vito.be (S.O.); dries.raymaekers@vito.be (D.R.)\\n*\\nCorrespondence: klaas.pauly@vito.be\\nAbstract: Drone-based photogrammetry typically requires the task of georeferencing aerial images\\nby detecting the center of Ground Control Points (GCPs) placed in the field. Since this is a very\\nlabor-intensive task, it could benefit greatly from automation. In this study, we explore the extent\\nto which traditional computer vision approaches can be generalized to deal with variability in real-\\nworld drone data sets and focus on training different residual neural networks (ResNet) to improve\\ngeneralization. The models were trained to detect single keypoints of fixed-sized image tiles with a\\nhistoric collection of drone-based Red–Green–Blue (RGB) images with black and white GCP markers\\nin which the center was manually labeled by experienced photogrammetry operators. Different\\ndepths of ResNets and various hyperparameters (learning rate, batch size) were tested. The best\\nresults reached sub-pixel accuracy with a mean absolute error of 0.586. The paper demonstrates\\nthat this approach to drone-based mapping is a promising and effective way to reduce the human\\nworkload required for georeferencing aerial images.\\nKeywords: drones; photogrammetry; ground control points; GCPs; RGB; computer vision; deep\\nlearning; ResNet; CNN\\n1. Introduction\\nThe utilization of camera-equipped Uncrewed Airborne Systems (UASs), commonly\\nreferred to as drones, within the realm of geomatics has experienced a notable surge in\\nrecent years. This can be attributed to advancements in sensor quality, cost reduction,\\nand enhanced integration, all of which have significantly improved accessibility and usabil-\\nity [1]. The raw data procured by UAS are typically used in a a meticulous photogrammetric\\nprocess to generate metric, survey-grade 3D deliverables. Equipped with Global Naviga-\\ntion Satellite Systems (GNSSs), these UASs capture center coordinates for each individual\\nimage acquisition point, with contemporary systems offering Real-Time Kinematic (RTK)\\ncorrection, thereby registering image positions at the centimeter level and providing better\\nconstraints to the photogrammetric algorithms.\\nThe photogrammetric process then consists of bundle adjustment, camera self-\\ncalibration, dense point cloud generation, orthorectification, and mosaicing of the images.\\nThese steps collectively rectify errors present in the initially recorded sensor positions\\nand interior and exterior sensor orientation values [2,3].\\nFigure 1 shows the photogrammetric process. Starting with bundle adjustment or\\nalignment, this step establishes connections between overlapping images by identifying\\nkeypoint features within images, using algorithms such as Scale-Invariant Feature Trans-\\nform (SIFT) [4–7] or Speeded-Up Robust Features (SURFs) [8]. These keypoint features are\\nRemote Sens. 2024, 16, 794. https://doi.org/10.3390/rs16050794\\nhttps://www.mdpi.com/journal/remotesensing\\nRemote Sens. 2024, 16, 794\\n2 of 27\\ninterlinked across images, culminating in a solution that explains the position and orienta-\\ntion (both interior and exterior) of cameras through tiepoints. Notably, bundle adjustment\\nand georeferencing may be susceptible to substantial errors, necessitating precise tiepoint\\nmeasurements. This is accommodated within the camera calibration step, where the bundle\\nadjustment process is reiterated based on the specification of pixel coordinates of Ground\\nControl Points (GCPs) of known real-world coordinates.\\nSubsequently, the densification of the point cloud is executed, whereby a 3D coordinate\\nis determined for every pixel within the image. Following this, a digital representation of\\nthe terrain is generated onto which orthorectified images are projected and mosaiced.\\nFigure 1. Simplified diagram of some steps for the photogrammetry workflow.\\nThe integration of GCPs is an indispensable facet of an accurate photogrammetric\\nprocess. Even when using RTK or post-processed GNSS-INS (Inertial Navigation System),\\na limited set of GCPs remains crucial for calibration purposes or for independent accuracy\\nassessments [2]. These markers, comprising easily identifiable shapes that stand out against\\nthe background, are strategically positioned on the ground. Their precise world coordinates,\\nacquired using survey-grade instruments such as RTK-GNSS or Post-Processed Kinematics\\n(PPK), are meticulously measured in the field [9]. A proper layout and measurement of\\nGCPs, as well as the precise indication of the point of measurement in the individual images\\n(labeling), has proven crucial in ensuring photogrammetric accuracy [10,11].\\nVarious types of GCP markers exist, predominantly consisting of contrasting surface\\ncolors such as black and white arranged in quadrants, while coded targets adhering to\\nrecognized standards can be detected by software through computer vision, this method is\\nmainly used in close-range photogrammetry and proves impractical for large-scale aerial\\nimagery. Challenges persist regarding the standardization of algorithms capable of au-\\ntonomously identifying non-coded GCP marker centers centers in drone-based applications.\\nConsequently, labeling is mostly performed manually to date [12]. This manual process\\nis arduous, time-intensive, prone to human error, and would significantly benefit from\\naccurate automation.\\nPrior attempts to automate GCP center point detection have employed methodologies\\nlike Edge Orientation Histograms [13], complemented by the integration of a Convolutional\\nNeural Network (CNN) to enhance robustness [14]. These efforts aimed to predict the inter-\\nsection point of 65 cm × 65 cm L-shaped GCPs. However, the accuracy of these approaches\\nwas susceptible to confounding features in the imagery, such as high altitudes, overlap\\nwith road stripes, or surrounding white dust, impacting their reliability. Other computer-\\nvision-based strategies involved X-marker detection using the Hough transform, achieving\\nan average sub-pixel distance of 0.51 between predictions and centers [15]. However, this\\nmethod operated on 12 × 12-pixel images extracted by a preliminary algorithm. A purely\\nRemote Sens. 2024, 16, 794\\n3 of 27\\ndeep learning-driven approach for keypoint detection might offer better generalization\\ncapabilities, accommodating irregularities in data without the prerequisite of an initial\\nalgorithm to extract the GCP center image.\\nAnother study devised to streamline the process of GCP detection involved the de-\\nvelopment of a comprehensive pipeline integrating deep learning techniques to detect a\\nbroad position of the marker and then compute the center point with computer vision\\nalgorithms such as the Canny edge detection method [16]. It found success in using in\\nsitu features such as road signs as substitutes for GCPs; however, such features do not\\ncommonly exist in agricultural environments. This research was motivated by the need for\\nTLS and photogrammetry data fusion, where two different types of control point patterns\\nare traditionally used. The model’s training utilized a small dataset comprising 246 images\\nwith fixed resolutions, showcasing prominently visible markers while exhibiting commend-\\nable performance under these conditions; the efficacy of this pipeline remains untested in\\nscenarios involving obscured markers, diverse lighting conditions, and variable resolutions.\\nIn practical agricultural applications, GCPs are conventionally installed and retained for\\nsuccessive UAS flights, potentially leading to differences in light conditions, resolutions,\\nand the quality of the markers, with all causing degradation in the control point pattern\\nappearance and necessitating the pipeline’s ability to generalize to predict GCPs amid\\nnon-ideal circumstances.\\nConvolutional Neural Networks (CNNs), a subset of Artificial Neural Networks\\n(ANNs), are commonly employed for analyzing visual imagery. Studies have investigated\\ncombinations of handcrafted filters like Harris [17] and Hessian [18] detectors with learned\\nCNNs—dubbed Key-Net—to detect keypoints, revealing that increasing CNN complexity\\ndoes not consistently enhance accuracy. Interestingly, integrating handcrafted filters signifi-\\ncantly reduces architectural complexity [19]. When it comes to keypoint matching across\\ndifferent raw images, CNNs outperform handcrafted descriptors in terms of accuracy amid\\nillumination and viewpoint changes [20]. Further comparison between pre-trained CNNs\\nand those trained from scratch suggests pre-trained CNNs fare better with illumination\\nchanges, while trained CNNs excel with viewpoint alterations.\\nDeeper CNNs face a degradation issue: as depth increases, accuracy plateaus, not\\ndue to overfitting but due to higher training error with additional layers. To tackle this,\\nResNet employs skip connections or shortcuts that leap over several layers [21]. Similar\\nto Long Short-Term Memory (LSTM) recurrent neural networks [22], there are two main\\nreasons why it is beneficial to add paths to skip connections in the model. First is the\\naforementioned accuracy saturation problem, and second, to avoid the issue of vanishing\\ngradients [23], thus resulting in more easily optimizable models. The gating mechanism\\nallows information to go through so-called information highways [24]. When comparing\\nResNet to the performance of other deep CNN models such as VGG16 [25] or Xception\\nnetwork [26], the ResNet model outperformed all others in classification tasks [27]. ResNet\\nhas been used in the context of GCP detection for classification purposes. In several recent\\nstudies, it has been used in combination with object detection architectures such as retinaNet\\nto accurately extract the bounding boxes of GCP pads [28,29]. Similar approaches have\\nbeen applied using other object detection architectures, such as object-oriented YOLOv5\\n[30]. The localization of the center point is performed by extracting the actual center of the\\nbounding boxes or utilizing edge features within them. While these methods can detect\\nGCP pads, including non-standard variants, with very high accuracy, the localization of\\ncenter points remains an issue in operational agricultural applications. In this context, GCP\\npads may be partly obscured or degraded, and such methods are likely to be inaccurate.\\nOriginally intended for classification, ResNet’s adaptability extends to regression tasks.\\nBy adjusting output layers and activation functions, ResNet transforms into a regression\\nmodel, evident in studies predicting pig weight from 3D images [31]. Specifically in the\\nfield of keypoint regression, ResNet has seen a lot of success in recent projects, such as in\\nkeypoint regression from a 2D image using image-level supervision [32]. Modifications of\\nmodels like Masked Loss Residual Convolutional Neural Network (ML-ResNet) have been\\nRemote Sens. 2024, 16, 794\\n4 of 27\\nused for facial keypoint detection with missing labels, producing satisfactory results [33],\\nas well as skeletal-point keypoint detection in single images [34]\\nThis paper aims to explore the resilience of a computer-vision-based approach us-\\ning real-world data. It proposes a ResNet-based deep learning pipeline specifically to\\nswiftly and accurately detect pixel coordinates of black and white ground control marker\\ncenter points. The paper suggests utilizing residual networks of various depths and com-\\nparing a wide array of training hyperparameters to identify an architecture capable of\\nachieving sub-pixel accuracy. The research will compare both deep learning and computer\\nvision approaches.\\n2. Materials and Methods\\n2.1. Existing Data Set\\nThis study started by assembling a curated dataset comprising 2980 JPEG images at\\ntheir original resolution, gathered between 2018 and 2021. The images were collected in the\\ncontext of several drone-based agricultural monitoring campaigns across fields in various\\ncountries, covering several crop types at different growth stages. All drone flights (474\\nin total) covered at least 5 GCP markers, and several drone and camera types were used,\\nresulting in image dimensions ranging from 2870 to 5460 pixels in width. Only images\\ncontaining at least one GCP marker were selected for the current dataset in the framework\\nof this research. For a comprehensive overview, Table 1 details the make and model of\\nall drones utilized in this study, alongside the total number of images captured by each\\ndrone during its flights, while Figure 2 shows the geographic and temporal distribution of\\ndrone flights. Target ground sample distances ranged from 2 to 10 mm, and the marker\\nsizes within the images varied between 80 and 700 pixels in length. Drones were flown at\\nheights of either 18 or 38 m, depending on target resolution. Figure 3 shows an example of\\na drone image containing a representative GCP marker in the dataset.\\nTable 1. Maker, model, and number of pictures taken by each drone used in the study.\\nMaker\\nPictures Taken\\nModel\\nPictures Taken\\nDJI\\nFC550\\n120\\nDJI\\nFC6310\\n1077\\nDJI\\nFC6310R\\n763\\nDJI\\nFC6310S\\n25\\nDJI\\nFC6510\\n79\\nDJI\\nFC6520\\n334\\nDJI\\nFC6540\\n132\\nDJI\\nM600_X5R\\n28\\nDJI\\nZenmuseP1\\n299\\nHasselblad\\nL1D-20c\\n24\\nSONY\\nDSC-RX1RM2\\n76\\nThe dataset exhibited variations in resolution and flight altitude, leading to diverse\\nsizes of Ground Control Points (GCPs) within the images. All individual images underwent\\nmanual annotation by proficient photogrammetry operators who used specialized software\\nto zoom in on the images and click on the GCP markers’ center point, thereby assigning a\\nsingle floating point image coordinate pair to the labeled point. Each operator meticulously\\nassigned a single keypoint, denoted by two floating point image coordinates in pixels,\\nprecisely at the center of each GCP.\\nRemote Sens. 2024, 16, 794\\n5 of 27\\nFlights per month\\nSite distribution by country\\n2018-05\\n2021-09\\n(a)\\n(b)\\nFigure 2. Distribution of sites surveyed (a), distribution of flights per month (b).\\nFigure 3. Example of a 6016 × 4008-pixel image captured with a FC6540 with 1/100 s of exposure\\ntime, 50 mm of focal length, and 2.97 max aperture.\\nThe dataset includes variability in environmental conditions and GCP marker quality,\\noriginating from the different flight locations and times. Figure 4 shows typical potential\\nissues that could impede center point detection, even for experienced human operators.\\nIn certain instances, the leaves and branches of crops near the control points might obstruct\\nthe UAS line of sight, as depicted in (d) of Figure 4. Additionally, environmental factors\\nlike wind might lead to the accumulation of sand or dead branches on the squares, ob-\\nscuring their features, similar to the example shown in (d) of Figure 4. Moreover, direct\\nsunlight reflecting off the GCP material into the camera could result in sun glare, causing a\\nblurred distinction between the white and black sections, as seen in (c) of Figure 4. Other\\nconfounding factors such as (limited) motion or focus blur can also occur.\\nRemote Sens. 2024, 16, 794\\n6 of 27\\nFigure 4. Examples of 512 × 512-pixel tiles, (a) a perfectly visible GCP, (b) GCP partially covered by\\nthe crop, (c) GCP reflecting sunlight, (d) GCP covered by sand.\\n2.2. Pre-Processing\\nDue to the dimensions and variable sizes of the entire individual images, training most\\ndeep learning models accurately for regression was infeasible. As a solution, the images\\nwere split into smaller, fixed sizes. A custom script was developed to extract tiles of a\\nspecified size encompassing the GCP center point. To prevent issues arising from tiles where\\nthe GCP center lay at the image edge, potentially impacting model accuracy, a padding\\nparameter was implemented. This padding ensured that if a situation occurred where the\\nGCP center was at the edge of a tile, an adjacent tile with better GCP visibility would cover\\nit. Consequently, this technique could generate multiple tiles per GCP, depending on the\\npadding amount.\\nThe initial execution of the script, focusing solely on images with black and white\\ncontrol points, aimed to generate 512 × 512-pixel tiles with a 25% padding (128 pixels). This\\nprocess resulted in 5658 tiles from the original 2980 images. Leveraging the human-labeled\\nimage pixel-coordinates for the GCP, the script efficiently determined tiles that undoubtedly\\ncontained a GCP. Moreover, the script provided the capability to include random tiles\\nwithout associated labels, yielding a subset of tiles without any GCPs. The discussion\\nsection outlines potential applications for these empty tiles.\\n2.3. Data Augmentation\\nThe diversity in drone flights offered a wide array of orientations of the ground control\\nmarkers in the images, with various lighting characteristics as a result. The tiling algorithm\\nemployed on full-resolution images also introduced variability in the GCP position within\\neach tile. The tiles covered instances of partially obscured control points, whether by sand,\\ncrop leaves, or material reflections. Initial testing revealed the model’s subpar performance\\nwith tiles predominantly occupied by the GCP. This might stem from factors such as flight\\naltitude, camera sensor specifications, resolution, and marker size. To address this issue\\nand create more instances where the GCP covered the entire tile, an additional set of tiles,\\nbeyond the initially generated ones (512 × 512, 25% padding), was computed.\\nThese new tiles featured a resolution of 224 × 224, aligning better with the require-\\nments of the ResNet model without requiring resizing, and maintained the same 25%\\npadding (56 pixels). This operation notably augmented the dataset by adding 5744 new\\ntiles for training, validation, and testing. Figure 5 illustrates an image that generated two\\n512 × 512 tiles, as the GCP center lay within the padding area along one side. Conversely,\\nthe generation of 224 × 224 tiles resulted in four distinct tiles since the GCP center was\\nsituated within a corner within the 25% padding.\\nIn essence, the marker predominantly occupied a larger portion of the smaller resolu-\\ntion tiles, providing the model with examples featuring larger GCPs and entirely new GCP\\ncenter locations. Notably, despite the example in Figure 5, the number of tiles produced by\\nthe tiling script remained unaffected by the tile size.\\nRemote Sens. 2024, 16, 794\\n7 of 27\\nFigure 5. Comparison of the same ground control point tiled in 512 × 512-pixel tiles, as presented by\\nfigure (a) and tiled in 224 × 224 pixel tiles as presented by figure (b).\\n2.4. Computer Vision Approach\\nThe determination of the edges of the ground control points involves a straightforward\\nprocess based on the formation of four squares. This method was implemented using the\\nCV2 Python library. Initially, the image was loaded (Figure 6a), and a binary mask was\\ncreated to highlight the white pixels (Figure 6b). This process yielded two square clusters\\nrepresenting the white tiles of the GCP. The parameters for generating this binary mask\\nare typically set between [255,255,255] and [240,240,240] (hexadecimal values ranging from\\n#FFFFFF to #F0F0F0), which generally provided satisfactory outcomes.\\nFigure 6. Visual representation of the computer vision pipeline, (a) original tile, (b) binary mask with\\nwhite pixels, (c) binary mask after morphological operations, (d) Canny edge detection, (e) Hough\\nLine Transform, (f) intersections of lines, (g) intersection of 90º angles, (h) average point.\\nTo minimize noise interference in the sample, morphological transformations were\\napplied to the mask to close internal gaps within the clusters. This step aimed to reduce\\nthe number of undetected pixels resulting from debris like sand or leaves covering the GCP.\\nCV2 offers functions to perform dilation followed by erosion for this purpose.\\nThe subsequent morphological transformation aimed to eliminate smaller clusters\\noutside the ground control points, which could arise from factors like white stones or\\nreflections in leaves. Utilizing CV2 functions for erosion followed by dilation, this operation\\nretained only the essential clusters. Both transformations employed a specific kernel size,\\nand adjustments to this kernel size would yield different outcomes. In the example, a 7 × 7\\nkernel was used (Figure 6c).\\nFollowing the isolation of the white tiles, detecting their edges ideally produces six\\nsegments forming two connected squares sharing a corner. This edge detection process\\nemploys the Canny edge detection algorithm [35] (Figure 6d). Modifiable parameters\\nRemote Sens. 2024, 16, 794\\n8 of 27\\nwithin these functions, notably two thresholds and the aperture size, allow for adjustments\\nto attain optimal results.\\nThe resulting image comprises two long segments intersecting in the center of the\\nGCP, this task is well suited for line detection algorithms. The Hough Line Transform [36]\\n(Figure 6e) operates by representing potential lines in the image as points in a two-parameter\\nspace: Rho, denoting the distance from the origin to the line along a normal vector, and Theta,\\nsignifying the angle of that normal vector. As the Hough transform algorithm computes\\nall feasible lines that could intersect each edge in the image, it saves these lines in an\\naccumulator matrix, a grid in the Rho–Theta space. The accumulation of votes in this matrix\\nresults in peaks that indicate regions where lines are most likely to exist in the image. It is\\nimperative during this process to filter and isolate only the lines passing through the center\\nof the GCP, disregarding those that intersect its edges.\\nAfter line detection, all intersections are computed and displayed as red dots in\\nthe image (Figure 6f). However, a challenge arises as the Hough Line Transform might\\ndetect multiple lines for the same GCP segment, leading to intersections distant from the\\nGCP center. To resolve this issue, angles between each pair of lines are calculated. Only\\nintersections originating from lines within a specified angle range, ideally close to 90º,\\nare considered.\\nEliminating intersections outside the [75º–105º] range refines the results (Figure 6g).\\nFinally, averaging all the remaining points yields the outcome of this approach (Figure 6h).\\nThis calculated average represents the center of the GCP based on this methodology.\\n2.5. Deep Learning Models\\nIn this paper, we tested three configurations of the ResNet model. By modifying\\nthe linear activation layer after the last bottleneck layer of the network with two output\\nneurons, for both the coordinates of the keypoint, it is possible to train such models for\\nregression. The two output neurons have a range of activation from 0 to 224 coinciding\\nwith the image-relative pixel coordinate of the center of the GCP after resizing both the\\ntiles and the pixel coordinates to fit the input neurons of the model. The architectures\\ntested for this study are ResNet-50, ResNet-101, and ResNet-152; these models are proven\\nto be more accurate by considerable margins in classification tasks than smaller ResNets,\\nsuch as ResNet-18 or even ResNet-34 [21]. All parameters in the ResNet architecture are\\ntrainable, and the number increases linearly with respect to the number of layers of the\\nmodel. Table 2 shows these specifications as well as the size the fully trained models have.\\nThis paper explores three configurations of the ResNet model by adjusting the linear\\nactivation layer post the final bottleneck layer. This modification involves incorporating\\ntwo output neurons specifically tailored for the coordinates of the keypoint. These models\\nare trained for regression, and the two output neurons are calibrated to have an activation\\nrange from 0 to 224. This range corresponds to the image-relative pixel coordinates of the\\nGCP center, after both the tiles and pixel coordinates have been resized to align with the\\ninput neurons of the model.\\nTable 2. Size and number of parameters for residual neural networks utilized.\\nModel\\nNumber of Parameters\\nSize of Model\\nResNet-50\\n23,512,130\\n277.289 MB\\nResNet-101\\n42,504,258\\n397.366 MB\\nResNet-152\\n58,147,906\\n674.413 MB\\nThe training pipeline was constructed using the PyTorch Python library. The architec-\\nture of the pretrained ResNets was sourced from the pretrainedmodels library available on\\nGitHub (https://github.com/Cadene/pretrained-models.pytorch, accessed on 21 February\\n2024). This repository aims to provide access to pretrained Convolutional Neural Net-\\nworks, aiding in the reproducibility of research paper results. Initial tests highlighted that\\nutilizing pretrained models instead of training from scratch resulted in superior accuracy.\\nRemote Sens. 2024, 16, 794\\n9 of 27\\nPrior research in image-related studies has consistently demonstrated the superiority of\\npretrained ResNets over their trained-from-scratch counterparts [37].\\nAll models underwent pretraining using the ImageNet dataset—a repository com-\\nprising over 14 million images and accessible to researchers for non-commercial use [38].\\nThe models were configured to accept torch tensors of 224 × 224 pixels and 3 RGB channels\\nas input. Prior to training, the dataset was resized in memory to align with the required\\nspecifications of the models.\\nFor the model evaluation process, three distinct loss functions were considered: mean\\nabsolute error (MAE) (Equation (1)), mean squared error (MSE) (Equation (2)), and smooth\\nL1 loss (SL1) (Equations (3) and (4)). The smooth L1 loss function incorporates a default\\nparameter beta, typically set to 1. This criterion employs a squared term if the absolute\\nerror falls below beta, otherwise utilizing the L1 norm. SL1 exhibits less sensitivity to\\noutliers and, in certain scenarios, mitigates issues related to exploding gradients compared\\nto MSE [39].\\nMAE = ∑n\\ni=1 |yi −ˆyi|\\nn\\n(1)\\nMSE = ∑n\\ni=1(yi −ˆyi)2\\nn\\n(2)\\nl(y, ˆy) = L = {l1, . . . , lN}T\\n(3)\\nli =\\n(\\n0.5(yi −ˆyn)2/beta\\nif |yi −ˆyi| < beta\\n|yi −ˆyi| −0.5 ∗beta\\nif otherwise\\n(4)\\nAn experiment was conducted to assess the effectiveness of different loss functions\\nused for training a single architecture. Three pretrained ResNet-50 models underwent\\ntraining for 50 epochs, employing a learning rate of 0.001 and a batch size of 32. These\\nmodels were trained using three distinct loss functions. Post-training, the models were\\nevaluated against an independent test set, and their performance was assessed using all\\nthree loss functions to determine the most accurate model. The outcomes of this experiment\\nare summarized in Table 3.\\nThe model trained with mean absolute error (MAE) exhibited superior performance\\nacross all three evaluation metrics, achieving approximately a 35% enhancement compared\\nto the model trained with smooth L1 loss (SL1). Notably, utilizing mean squared error\\n(MSE) as the loss function for the ResNet resulted in the least accurate predictions among\\nthe three metrics assessed.\\nTable 3. Errors on predictions of the test set of models trained with different loss functions.\\nModel Trained with\\nMean Absolute Error\\nMean Squared Error\\nSmooth L1\\nMAE\\n0.858\\n1.405\\n0.481\\nMSE\\n3.074\\n21.338\\n2.606\\nSL1\\n1.237\\n2.788\\n0.831\\nThe training pipeline employed the Adam optimizer [40], selected for its efficacy\\nin optimizing stochastic objective functions based on adaptive estimates of lower-order\\nmoments. Notably, this optimizer requires minimal tuning and is ideal for first-order\\ngradient-based optimization. Preliminary experiments indicated that all models converged\\nbefore reaching the 50th epoch, signifying efficient convergence rates. Table 4 presents the\\nshared hyperparameters utilized across all trained models in this study.\\nRemote Sens. 2024, 16, 794\\n10 of 27\\nTable 4. Common hyperparameters utilized to train all models.\\nHyperparameters\\nValue\\nTraining epochs\\n50\\nLoss function\\nMAE\\nTraining slip\\n≈70%\\nValidation slip\\n≈15%\\nTest slip\\n≈15%\\nTransfer learning\\nImageNet\\nActivation function\\nLinear\\nOptimizer\\nAdam\\nAll experiments were executed using Jupyter notebooks within the Google Colab\\nplatform. The allocated Graphics Processing Unit (GPU) provided by the platform was\\nthe NVIDIA Tesla T4, equipped with approximately 12.6 GB of Random-Access Memory\\n(RAM) and 33 GB of available disk space. Additionally, the Central Processing Unit (CPU)\\nallocated for computation was the Intel Xeon CPU @ 2.20GHz.\\n2.6. Experimental Setup\\nThe complete raw dataset was utilized to generate two types of tiles: 224 × 224 and\\n512 × 512 pixels. These tiles were then distributed across a test set and a validation set,\\nwith each set containing 15% of the tiles. The remaining 70% constituted the training set.\\nThe primary focus of the experiments centered on identifying optimal hyperparameters\\nfor model training and comparing various ResNet architectures—ResNet50, ResNet101,\\nand ResNet152. A total of 36 models were trained for the principal experiment, encom-\\npassing combinations of 3 learning rates ([0.01, 0.001, 0.0001]) and 4 batch sizes (16, 32, 64,\\nand 96) across the 3 architectures.\\nDuring training, the validation split was employed to calculate the model’s accuracy\\non each epoch based on the mean absolute error (MAE) criterion. Upon reaching a new\\nminimum validation loss, the model was saved, ensuring that the stored model represented\\nthe instance with the lowest validation loss within 50 epochs.\\nPerformance evaluation on the test sets was conducted using both mean absolute error\\nand mean squared error metrics. The evaluation encompassed the complete test set, as well\\nas test sets comprising only 224 × 224 and 512 × 512 tiles individually. Figure 7 visually\\nillustrates the workflow of this study.\\nFigure 7. Workflow of the main steps performed in this study.\\nRemote Sens. 2024, 16, 794\\n11 of 27\\n3. Results\\n3.1. Computer Vision Approach\\nInitial testing revealed that the Rho parameter within the Hough Line Transform\\noperator significantly impacted the results. In this function, the Rho parameter determines\\nthe granularity of the accumulator array in the Hough transform. A smaller Rho value\\nresults in a finer resolution of the accumulator matrix, potentially enhancing the detection\\nof lines at finer resolutions and smaller gaps within the image. Conversely, increasing the\\nrho value coarsens the resolution of the accumulator matrix, accelerating the algorithm\\nbut potentially limiting its ability to detect lines at finer resolutions or with subtle details.\\nFigure 6 demonstrates a prediction with an absolute distance of approximately 1.96 pixels,\\nspecifically observed with a Rho value set to 1.15. Conversely, Figure 8 showcases three\\nadditional examples where the computer vision-based approach predicted the point within\\na reasonable distance from the human-labeled point. Notably, the optimal Rho parameter\\nvaried for each of these images and was specifically selected to optimize the predictions.\\nFigure 9 presents another tile computed using the same hyperparameters, as demon-\\nstrated in Figure 6. In this case, the Hough Line Transform operation erroneously detects\\nlines along the outer edges of the white squares. This detection generates crossing points\\nin all four corners, consequently shifting the average point away from the intended target\\npoint. This discrepancy results in a considerable difference of 75.56 pixels between the\\npredicted and desired points. In more extreme scenarios, the Hough Transform may fail\\nto detect any lines or produce angles between the lines that deviate significantly from 90º,\\nleading to a lack of detected intersections and, subsequently, no point prediction.\\nFigure 8. Visualization of the computer vision algorithm predicting the center points of different\\nGCPs with the parameters utilized and the absolute error between the predicted point and the\\nhuman-labeled point.\\nFigure 9. Example of the computer vision pipeline with non-optimal parameters for the tile (a) un-\\naltered tile, (b) Canny edge detection, (c) crossing points, red points for intersections of lines, blue\\npoint as the average crossing point.\\nRemote Sens. 2024, 16, 794\\n12 of 27\\nTo investigate the impact of the Rho parameter on the Hough Transform and its\\neffect on the computer vision approach’s performance, an experiment was devised. This\\nexperiment involved generating predictions for 1000 512 × 512-pixel tiles while varying\\nthe Rho parameter from 0.5 to 3.5. Figure 10 illustrates the outcomes of this experiment\\nacross three graphs.\\nThe first graph (a) in Figure 10 illustrates the number of unpredicted tiles against the\\nRho parameter. Lower Rho values result in fewer Hough Lines detected. Consequently,\\nif no lines or their intersections have acute angles, no point prediction is made. With a\\nstarting Rho value of 0.5, over 85% of the images resulted in no prediction. However, as Rho\\nincreases, this proportion decreases, dropping below 40% at Rho = 3.5.\\nThe second graph (b) in Figure 10 illustrates the Mean Absolute Error (MAE) for the\\ncomputer vision pipeline against the Rho parameter. Variations in the parameter produce\\nminimal changes in the MAE, maintaining a relatively consistent value of approximately\\n75 throughout the experiment.\\nLastly, the third graph (c) in Figure 10 depicts the time required to predict the thousand-\\nimage batch against the Rho parameter. Beginning around 15 s with Rho = 0.5, the time\\nexponentially increases with higher Rho values, reaching over 15 min with Rho = 3.5.\\nNotably, there exists a substantial variance in computational time among individual images.\\nImages that yield numerous lines necessitate extensive computational effort to compute all\\nintersections between them.\\nFigure 10. Graphs representing the results of testing the computer vision pipeline on a thousand\\nimages with different Rho values for the Hough Line Transform. (a) Number of unpredicted tiles vs.\\nRho, (b) MAE vs. Rho, and (c) time to predict 1000 images vs. Rho.\\nRemote Sens. 2024, 16, 794\\n13 of 27\\n3.2. Deep Learning Approach: Influence of Tile Size\\n3.2.1. Influence with Training\\nThe depicted graph in Figure 11 illustrates the outcomes of training using the chosen\\ndata augmentation technique compared to training solely with one tile size resolution.\\nThe model trained with a combination of tiles showcased significant performance gains.\\nSpecifically, when tested against 224 × 224-pixel tiles, it outperformed the rest by over\\n60%. Additionally, it showcased a 25% improvement with 512 × 512-pixel tiles and an\\nexceptional 80% enhancement with the combined dataset of both resolutions. In contrast,\\nthe performance of the dataset with the smaller tile size was notably poor compared to the\\nhigher tile size or augmented datasets.\\nFigure 11. Mean absolute error of three ResNet-50 models trained for 50 epochs with: 224 × 224-pixel\\ntiles, 512 × 512-pixel tiles and a mix of both. Learning rate of 0.001 and a Batch Size of 32. Results\\ndisplayed against a test set containing only 224 × 224-pixel tiles (orange), 512 × 512-pixel tiles (light\\nblue), and a mix of both (dark blue).\\n3.2.2. Influence with Testing\\nThe graph presented in Figure 12 offers a comparative analysis of the Mean Absolute\\nError (MAE) results for all three architectures. These models were trained using a batch\\nsize of 32 and a learning rate of 0.001, utilizing test sets composed of 224 × 224-pixel\\ntiles, 512 × 512 pixel tiles, and a combination of both, all sourced from the augmented\\ndataset. Across the board, the results indicate that the models performed less accurately\\non the test sets comprising smaller tiles in comparison to the 512 × 512-pixel tiles and\\nthe augmented set. Notably, the test set involving mixed tile sizes consistently exhibited\\nsuperior performance throughout all three models when compared to the other test sets.\\nIn Figure 13, an extensive comparison of the Mean Absolute Error (MAE) across all\\ntrained models is presented against the three distinct test sets. These findings reinforce the\\ntrends observed in previous experiments. Specifically, the MAE scores from the 512 pixel\\ntest set were consistently, on average, 71.388% lower than those obtained from the 224 pixel\\ntest set.\\nOnce again, the test set consisting of a mix of tile sizes demonstrated superior perfor-\\nmance across all models. On average, its Mean Absolute Error was approximately 80.513%\\nlower than that of the 224-pixel test set and exhibited a 30.113% reduction compared to the\\n512-pixel test set. These findings solidify the superiority of the model trained on mixed tile\\nsizes in comparison to the single tile-size-based models.\\nRemote Sens. 2024, 16, 794\\n14 of 27\\nFigure 12. Mean absolute error of three models: ResNet-50, ResNet-101 and ResNet-152, trained\\nfor 50 epochs with a mix of 224 × 224- and 512 × 512-pixel tiles, with a learning rate of 0.001 and a\\nBatch Size of 32. Results displayed against a test set containing only 224 × 224-pixel tiles (orange),\\n512 × 512-pixel tiles (light blue), and a mix of both (dark blue).\\nFigure 13.\\nComparison of mean absolute error of all trained models in order against a test set\\ncomposed of 224 × 224-pixel tiles, 512 × 512-pixel tiles and a mix of both. Each color represents a\\nsingle ResNet model with specific hyperparameters.\\nTable 5 provides an overview of the average MAE across different image-size test sets\\nfor each studied architecture (ResNet50, ResNet101, and ResNet152). Consistently, the data\\nalign with the previous observation highlighting the superior performance of the mixed\\ntile models.\\nAcross the architectures, performance is notably uniform, showcasing similar MAE\\nvalues. However, ResNet152 exhibited superior performance in predicting center points\\nwithin the 224 × 224-pixel tiles compared to the other architectures. In contrast, ResNet101\\ndemonstrated relatively poorer performance in the 512 × 512-pixel tiles compared to\\nits counterparts.\\nIt is important to note that all trained models underwent testing against a diverse test\\nset containing a mix of 224 × 224- and 512 × 512-pixel tiles.\\nRemote Sens. 2024, 16, 794\\n15 of 27\\nTable 5. Average mean absolute error across all trained models on different ResNet architectures with\\ndifferent test sets: 224 × 224-pixel tiles, 512 × 512-pixels tiles, and a mix of both.\\nArchitecture\\nMAE for 224 × 224\\nMAE for 512 × 512\\nMAE for Mix Tiles\\nResNet50\\n13.732\\n2.775\\n2.038\\nResNet101\\n11.272\\n3.309\\n2.099\\nResNet152\\n10.365\\n2.771\\n2.215\\n3.3. Deep Learning Approach: Main Results\\nFigure 14 outlines the performance outcomes for various batch sizes and learning rate\\ncombinations specific to ResNet50. Similarly, Figures 15 and 16 depict the corresponding\\nresults for ResNet101 and ResNet152, respectively. Notably, Figure 16 lacks results for\\nmodels trained with a batch size of 96. This omission arises due to the substantial memory\\nrequirements (as detailed in Table 2) for ResNet152 models and the memory limitations of\\nthe Google Colab environment (approximately 12.6 GB), rendering the use of such a batch\\nsize unfeasible within this experimental setup.\\nThe impact of hyperparameter configurations on model accuracy is evident from the\\npresented results. The most optimal performance was achieved by ResNet152, yielding an\\nMAE of 0.723 when trained with a batch size of 32 and a learning rate of 0.0001. Following\\nclosely, ResNet101 attained an MAE of 0.746 with a batch size of 64 and a learning rate\\nof 0.0001, securing the second-best performance. ResNet50, utilizing default parameters,\\nobtained an MAE of 0.858 with a batch size of 32 and a learning rate of 0.001, positioning it\\nin third place among the tested configurations.\\nResNet 50\\nFigure 14. Accuracy (MAE) of ResNet50 with different hyperparameters (batch size and learning\\nrate) after 50 Epochs of training.\\nResNet 101\\nFigure 15. Accuracy (MAE) of ResNet101 with different hyperparameters (batch size and learning\\nrate) after 50 Epochs of training.\\nRemote Sens. 2024, 16, 794\\n16 of 27\\nResNet 152\\nFigure 16. Accuracy (MAE) of ResNet152 with different hyperparameters (batch size and learning\\nrate) after 50 Epochs of training.\\nTable 6 shows the mean MAE and MSE across all three architectures as well as their\\nstandard deviations, while ResNet101 performed best overall, all models scored on average\\naround 2 pixels MAE.\\nTable 6. Mean MAE and MSE as well as the standard deviation for the results of all trained models\\nin all three architectures.\\nArchitecture\\nMean MAE\\nStandard Deviation\\nfor MAE\\nMean MSE\\nStandard Deviation\\nfor MSE\\nResNet50\\n2.133\\n1.265\\n12.061\\n12.915\\nResNet101\\n1.841\\n1.116\\n9.180\\n10.439\\nResNet152\\n2.215\\n1.172\\n14.032\\n14.473\\nThe influence of the learning rate hyperparameter on the model performance is notably\\nevident. As demonstrated in Table 7, reducing the learning rate correlates with enhanced\\naccuracy. This table presents the mean values for both MAE and MSE, accompanied by\\ntheir respective standard deviations, showcasing a consistent reduction in these metrics as\\nthe learning rate decreases. Similarly, Table 8 explores the impact of varying batch sizes\\non model performance, portraying MAE and MSE alongside their standard deviations.\\nInterestingly, while mean values hover around 2 for MAE and 10 for MSE across different\\nbatch sizes, there appears to be no direct correlation between altering the batch size in\\nisolation and achieving improved performance.\\nTable 7. Mean MAE and MSE as well as the standard deviation for the results of all trained models\\nin all three studied learning rates.\\nLearning Rate\\nMean MAE\\nStandard Deviation\\nfor MAE\\nMean MSE\\nStandard Deviation\\nfor MSE\\n0.01\\n3.475\\n0.716\\n26.328\\n9.477\\n0.001\\n1.505\\n0.516\\n5.860\\n4.998\\n0.0001\\n1.167\\n0.341\\n2.463\\n1.437\\nTable 8. Mean MAE and MSE as well as the standard deviation for the results of all trained models\\nin all three studied batch sizes.\\nBatch Size\\nMean MAE\\nStandard Deviation\\nfor MAE\\nMean MSE\\nStandard Deviation\\nfor MSE\\n16\\n2.083\\n0.831\\n12.877\\n11.618\\n32\\n1.929\\n1.228\\n10.446\\n12.340\\n64\\n1.995\\n1.244\\n10.752\\n12.975\\n96\\n2.260\\n1.596\\n12.415\\n15.187\\nRemote Sens. 2024, 16, 794\\n17 of 27\\n3.4. Deep Learning Approach: Individual Model Analysis\\nFor validation of model convergence throughout training, both the training and\\nvalidation MAE losses were recorded per epoch. Among these, the model chosen was the\\none displaying the lowest validation loss within the 50 epochs. Figure 17 visualizes the\\nprogression of training and validation losses across 50 epochs for each architecture. All\\nmodels reach convergence well before the 35th epoch.\\nDisplayed in Figure 18 are frequency distribution plots portraying the disparity be-\\ntween human-labeled and model-predicted X and Y coordinates. These plots focus on the\\nthree most effective models derived from the array of experiments conducted. A slight bias\\nis noticeable towards a positive X coordinate (indicating predictions skewed to the right of\\nthe center point). Specifically, ResNet-50 model showcased an average X coordinate error\\nof 1.14 pixels, while ResNet-101 displayed 0.33 pixels, and ResNet-152, 0.70 pixels. Corre-\\nspondingly, their standard deviations were 6.19, 6.52, and 7.17, respectively. To address this\\nbias, an experiment was executed, extending the training of these top-performing models\\nby an additional 150 epochs each.\\n(a)\\n(b)\\n(c)\\nFigure 17. Graphs comparing the training loss MAE (orange) and validation loss MAE (red) on\\ndifferent models across 50 epochs of training. (a) ResNet 50: learning rate = 0.01, batch size = 64,\\n(b) ResNet 101: learning rate = 0.001, batch size = 96 and (c) ResNet 152: learning rate = 0.01, batch\\nsize = 16.\\nRemote Sens. 2024, 16, 794\\n18 of 27\\n(a)\\n(b)\\n(c)\\nFigure 18. Frequency distribution plot for the difference between the human-labeled and model-\\npredicted point coordinates (X red and Y blue). Three architectures trained for 50 epochs are displayed:\\n(a) ResNet 50: learning rate = 0.001, batch size = 32, (b) ResNet 101: learning rate = 0.0001, batch\\nsize = 64 and (c) ResNet 152: learning rate = 0.001, batch size = 32.\\nIn Figure 19, the outcomes of the extended training experiment are depicted. With the\\nadditional training, there is a marginal reduction in the mean absolute error (MAE) for\\nall models, alongside a significant decrease in the bias toward the X coordinate. Post-\\nextension, ResNet-50 exhibits an average error of 0.62 for the X coordinate, while ResNet-\\n101 demonstrates −0.18, and ResNet-152 shows −0.06. Their respective standard deviations\\nfor this metric are 6.18, 9.18, and 9.06. In Table 9, the MAE for models after undergoing\\nfour times the initial epochs is tabulated, showcasing a notable reduction in losses across\\nthe board, with ResNet-152 achieving a value lower than 0.6.\\nFigure 20 shows some examples of the top 50% predictions of the best performing\\nmodel (ResNet-152: batch size = 32, learning rate = 0.0001 trained for 200 Epochs). The red\\ndot represents the prediction of the model and the blue the human-labeled center of the\\nGCP, if both points are in close vicinity the red one will appear on top. Predictions in\\nproblematic tiles are displayed in the examples, (b) and (d) Figure 20 show control points\\npartially covered by leaves and sand. (a) Figure 20 shows a tile that is comprised almost\\nentirely by the marker. (e) Figure 20 shows sun glare occurring, (c) Figure 20 is a marker\\nwhose center is on the edge of the tile and (f) Figure 20 is a smaller marker due to a higher\\nresolution image.\\nRemote Sens. 2024, 16, 794\\n19 of 27\\n(a)\\n(b)\\n(c)\\nFigure 19. Frequency distribution plot for the difference between the human-labeled and model-\\npredicted point coordinates (X red and Y blue). Three architectures trained for 200 epochs are\\ndisplayed: (a) ResNet-50: learning rate = 0.001, batch size = 32, (b) ResNet-101: learning rate = 0.0001,\\nbatch size = 64 and (c) ResNet-152: learning rate = 0.001, batch size = 32.\\nTable 9. Comparison of the MAE for the top three performing models after being trained for 50 and\\n200 epochs. (ResNet-50: learning rate = 0.001, batch size = 32, ResNet-101: learning rate = 0.0001,\\nbatch size = 64 and ResNet-152: learning rate = 0.0001, batch size = 32.)\\nArchitecture\\nMAE after 50 Epochs\\nMAE after 200 Epochs\\nResNet-50\\n0.858\\n0.664\\nResNet-101\\n0.746\\n0.632\\nResNet-152\\n0.721\\n0.586\\nRemote Sens. 2024, 16, 794\\n20 of 27\\nFigure 20. Examples of the top 50% predictions of the ResNet-152 trained for 200 epochs with batch\\nsize = 32 and learning rate = 0.0001. The blue point represents the human-labeled keypoint and the\\nred point represents the prediction\\nIn Figure 21, three examples of the worst 5% predictions are showcased to study the\\nmodel’s limitations. Instance (a) illustrates an extreme scenario with dense vegetation\\nobscuring the tile, while in (b), environmental conditions and image acquisition parameters\\nresulted in image blurring, making the center cross of the marker indistinct. In tile (c),\\nthe marker’s center lies on the image’s edge, leading to inaccurate center detection by the\\nmodel. Conversely, within the bottom 5% of predictions, some instances exhibit the model’s\\nhigher accuracy compared to the human-labeled center point. These cases are portrayed in\\nFigure 22.\\nFigure 21. Examples of the bottom 5% predictions of the ResNet-152 trained for 200 epochs with\\nbatch size = 32 and learning rate = 0.0001. The blue point represents the human-labeled center point\\nand the red point represents the prediction\\nRemote Sens. 2024, 16, 794\\n21 of 27\\nFigure 22. Three prediction examples with human error by the ResNet152 batch size = 32, learning\\nrate = 0.0001, the blue point represents the human-labeled center point and the red point represents\\nthe prediction. Images (a,c) are 512 × 512-pixel tiles, and (b) is 224 × 224-pixel tiles.\\n3.5. Deep Learning: Training Time\\nRegarding the model training time, Table 10 shows the average time it took to train\\neach of the different models for 50 epochs with all configurations. A deeper ResNet model\\nmeans a higher number of parameters to train and backpropagate (Table 2) and, with this,\\na longer training time.\\nTable 10. Average training time for 50 epochs for all tester architectures.\\nArchitecture\\nAverage Training\\nTime (s)\\nAverage Training\\nTime (HH:MM:SS)\\nAverage Seconds\\nper Epoch\\nResNet-50\\n6559.116\\n01:49:19\\n131.182\\nResNet-101\\n9802.033\\n02:43:22\\n196.040\\nResNet-152\\n11,893.568\\n03:18:13\\n237.871\\n3.6. Deep Learning: Prediction Time\\nTo test the inference time of the different models, a number of test set (1200 tiles)\\npredictions were carried out. Table 11 shows the results of computing the center of the\\ncontrol points utilizing only the CPU (Intel Xeon CPU @ 2.20GHz) and utilizing also a GPU\\n(NVIDIA Tesla T4). Similar to the training times, the deeper the model, the longer it takes\\nfor the inference. The inference time when using GPU is less than 3% of the inference time\\nusing only CPU.\\nTable 11. Average time of prediction of 1200 tiles with batch size = 32 for different architectures with\\nonly a CPU and with the use of a GPU.\\nArchitecture\\nAverage\\nSeconds CPU\\nAverage\\nMin:Sec CPU\\nAverage\\nSeconds GPU\\nAverage\\nMin:Sec GPU\\nResNet50\\n238.083\\n03:58\\n6.687\\n00:07\\nResNet101\\n401.205\\n06:41\\n7.482\\n00:07\\nResNet152\\n582.306\\n09:42\\n9.415\\n00:09\\n4. Discussion\\nThis study aimed to explore a deep learning-driven methodology for accurately\\nlocalizing the center of black and white squared Ground Control Points (GCPs) from RGB\\nimages captured by drones. We conducted a comparative analysis involving three ResNet\\narchitectures (ResNet-50, ResNet-101, and ResNet-152) with varying hyperparameters.\\nRemote Sens. 2024, 16, 794\\n22 of 27\\nThe ResNet-152 model, trained with a learning rate of 0.0001 and a batch size of 32,\\ndemonstrated the most favorable performance, achieving a mean average error (MAE) of\\n0.586 pixels, indicating sub-pixel accuracy. Comparable results were observed with other\\narchitectures, with the ResNet-50 and ResNet-101 achieving MAEs of 0.664 pixels and\\n0.632 pixels, respectively.\\nStudies relying solely on a computer vision approach, employing methods like the\\nHough transform, have demonstrated a precision level averaging 0.51 pixels in discerning\\ncenters compared to reference centers [36]. However, these methodologies were executed\\non small 12 × 12 pixel tiles exclusive to markers. Implementing such an approach in\\nreal-world scenarios necessitates an initial algorithm to accurately extract these centers,\\na method not detailed by the authors of this study. Without precise extraction of markers\\nfrom images, this approach might lack applicability in diverse drone imagery contexts.\\nIn our investigation, we experimented with a computer vision pipeline leveraging the\\nHough transform, akin to the reference study [36]. Unlike the referenced methodology, our\\npipeline processed complete 512 × 512-pixel tiles encompassing substantial background\\nalong with the Ground Control Points (GCPs), while this algorithm yielded reasonably\\naccurate outcomes, its heavy reliance on parameter settings poses a significant challenge.\\nFine-tuning parameters becomes imperative for individual images, accommodating varia-\\ntions in lighting conditions, material coverage over control points and resolution disparities,\\namong other factors. Consequently, this approach lacks the capability to generalize for\\nautomation. Modifying parameters such as the Rho value in the Hough Line Transform\\ndoes not enhance overall performance but substantially reduces the number of unpredicted\\ntiles at the expense of exponentially elongated prediction times for the pipeline.\\nAn alternative study introduced an approach using edge-oriented histograms in con-\\njunction with a modified Canny edge detection, coupled with a CNN [14]. This method\\neffectively segmented GCP markers, accommodating variations in scale, rotation, and il-\\nlumination. However, the pipeline demonstrated shortcomings when faced with image\\nirregularities, such as unclear marker visibility, the presence of other white objects proxi-\\nmate to or beneath the GCP, and the influence of shadows on the GCP.\\nStudies using deep learning for broad GCP location and computer vision for center de-\\ntection [16] are effective within the scope of controlled conditions. However, this method’s\\nreliance solely on computer vision algorithms for edge detection is susceptible to failure in\\nscenarios less conducive to ideal conditions, as evidenced by the findings in this study.\\nIn contrast, the deep learning approach adopted in our study exhibits robustness\\nagainst these data irregularities (refer to Figure 20). By training the model with less distinct\\nexamples, it gains the capacity to discern GCPs across a wide spectrum of scenarios, offering\\nenhanced adaptability to various environmental conditions.\\nThe augmentation of the dataset through the inclusion of varied tile sizes has proven\\nto be a potent strategy in enhancing the accuracy of diverse models. Upon comparing the\\nperformance of ResNet architectures (ResNet-50, ResNet-101, and ResNet-152), it is evident\\nthat all these architectures are capable of achieving sub-pixel Mean Absolute Error (MAE)\\nagainst an independent test set, provided their hyperparameters are appropriately tuned.\\nWhile ResNet-152 emerged as the top-performing model after 50 epochs of training,\\nachieving a modest increase of approximately 2.5% in accuracy over the best ResNet-101\\nmodel, this improvement comes at the expense of a larger model size (refer to Table 2).\\nMoreover, employing ResNet-152 entails roughly 20% longer training times (as indicated\\nin Table 10). Additionally, the computational resources required for predictions are notably\\nimpacted by deeper models; transitioning from ResNet-101 to ResNet-152 architecture\\nincreases the prediction time by approximately 30% when relying solely on CPU computa-\\ntions, although this effect is significantly mitigated when utilizing a GPU.\\nChoosing the appropriate model for a specific pipeline should consider the technical\\nconstraints and logistical implications of the project, while 50 epochs generally suffice\\nfor model convergence, further training can marginally enhance accuracy while reducing\\nbiases in error along any axis.\\nRemote Sens. 2024, 16, 794\\n23 of 27\\nIt is straightforward to reproduce the methodology delineated in this study concern-\\ning data preprocessing, augmentation, and machine learning predictions. Paramount to\\nthe successful deployment of this framework is the acquisition of an extensive, human-\\nannotated dataset meticulously capturing the myriad of scenarios anticipated in real-world\\napplications. Variables such as drone flight altitudes, diverse lighting conditions, GCP\\nmarker characteristics and potential obstructions on GCPs among other factors stand as\\npivotal components necessitating representation within this dataset. The comprehensive-\\nness and fidelity of this dataset significantly underpins the robustness and adaptability of\\nthe proposed pipeline to diverse real-world conditions. Further, adequate finetuning of the\\nhyperparameters can also significantly influence model performance.\\nThis study exclusively focused on black and white-squared GCPs, leaving scope for\\nfurther exploration regarding the transferability of the model to recognize other shapes and\\nmaterials of GCPs. The models were specifically trained to detect the image-relative pixel\\ncoordinates of GCPs within tiles of limited, fixed size. Future investigations should delve\\ninto methodologies to identify the pixel coordinates of GCP centers within variably-sized\\nentire images.\\nIt is generally preferred by drone operators to fly in clear sky conditions, however due\\nto weather and time constraints this isn’t always possible. Stable conditions are otherwise\\npreferred, to limit the appearance of clouds on the final orthomosaic. Limited instances\\nof spotty cloud directly over GCP targets occurred in the dataset, and so performance\\nin this regard has not been evaluated. For the purpose of model training, additional\\ndata augmentation to synthetically alter illumination can lead to better generalization in\\ndiffering flight conditions.\\nPotential avenues for exploration could involve integrating a classifier to identify tiles\\ncontaining GCPs and subsequently performing keypoint regression exclusively on those\\ntiles. Alternatively, employing object detection on entire images to focus tiling efforts solely\\non areas where GCPs are detected could be explored. Further research could explore the\\nutilization of the You Only Look Once (YOLO) architecture [41], a state-of-the-art, real-time\\nobject detection system. YOLOv7, in particular, has demonstrated high accuracy and speed\\nacross various scenarios [42], and its applicability for pose estimation via keypoint detection\\nmakes it a promising avenue to explore. Investigating a rapid, single-step approach to\\ndetect both the GCP marker and its center point using this model could be a valuable\\npursuit. Additionally, recently released multimodal large language models (LLMs) such as\\nGPT-4V, Mixtral 8x7B and CogVLM are worth exploring for their capabilities in GCP center\\npoint detection, based on image tiles small enough to be ingested for inference. The same\\nmodels can potentially be used to first look for GCP markers in entire images before\\ntiling and selecting the relevant tiles for center point inference. Particularly, investigating\\nimprovements based on Set-of-Mark prompting using output from segmentation such as\\nSegment Anything Model (SAM) holds potential to fast-track this process, especially as the\\nhardware requirements of running inference based on LLMs can still present a bottleneck.\\nIn practice, the model developed in this study will be integrated into an operational\\nworkflow. Its predictions will be cross-validated against further human annotations. The ul-\\ntimate goal is to automate the generation of accurate, geo-referenced spatial datasets,\\nrendering the process more precise and faster than manual intervention. This move to-\\nwards automation seeks to replace human annotators, ensuring efficient and accurate\\nprocessing of spatial data.\\n5. Additional Content\\nTile Classifier\\nIn this paper, a method to detect the centers of black and white squared GCP markers\\nis outlined, albeit not encompassing the entire pipeline required for handling variable\\nresolution images. As a proof of concept, a machine learning tile classifier was developed\\nto differentiate tiles containing GCPs from those without.\\nRemote Sens. 2024, 16, 794\\n24 of 27\\nDue to time constraints and this aspect not being the primary focus of the study, Mi-\\ncrosoft Lobe was utilized to construct the model. Lobe is a desktop application facilitating\\nthe creation, management, and utilization of custom machine learning models for image\\nclassification. Training data, obtained through the tiling script, consisted of approximately\\n10,000 512 × 512-pixel tiles, with 60% devoid of GCPs and 40% including them. The ap-\\nplication offers real-time control over the training process. Specifically, the model within\\nLobe can be optimized for speed, in which case MobileNetV2 is used, or for accuracy,\\nin which a ResNet50-V2 is trained. Lobe automatically takes care of data augmentation in\\nthe background to improve training accuracy. Once an acceptable accuracy level is achieved,\\nthe model can be tested within the application by introducing new, unlabeled examples.\\nAdditionally, misclassified examples can be utilized for further training. Although the\\nmodel was trained exclusively using 512 × 512-pixel tiles, Lobe supports training with\\nvariable-sized images.\\nOnce the model was trained, it was exported as a protobuf file for integration with\\nTensorFlow in Python3, enabling its incorporation into the broader pipeline. The Lobe\\ndesktop application purported a 98% accuracy in classification, albeit when tested against\\ntiles it had already encountered. To validate its performance further, a new evaluation\\nwas conducted involving approximately 7000 unseen 512 × 512-pixel tiles—60% devoid\\nof GCPs and 40% containing them. Table 12 demonstrates the outcomes of this assess-\\nment, indicating a close to 97% accuracy in classifying unseen tiles, both those with and\\nwithout GCPs.\\nTable 12. Results of the Lobe trained classifier.\\nStatistic\\nPercentage\\nGCP classified as GCP\\n96.8034%\\nGCP classified as Empty\\n3.1965%\\nEmpty classified as Empty\\n96.5290%\\nEmpty classified as GCP\\n3.4709%\\nTotal Accuracy\\n96.6847%\\nBased on the visual inspection of misclassified examples, it appears that many misclas-\\nsifications occur when the GCP marker is positioned near the edges of the image, similar to\\nthe scenario depicted in image (c) in Figure 21. Additionally, shapes sharing structural and\\ncolor resemblances with the markers may lead to erroneous classifications.\\nFor a comprehensive view of the tile detection process and an illustration of a mis-\\nclassified tile, Figure 23 provides a detailed depiction. The image is initially divided into\\n512 × 512 pixel tiles, each of which passes through the TensorFlow model. Tiles tinted in\\nblue represent those identified as GCP-containing tiles. In one instance, the entire GCP\\nmarker is correctly identified within a single tile, while the surrounding tiles, despite\\npadding, do not encompass the marker’s center, correctly classified as non-GCP. However,\\nan adjacent tile contains a signpost casting a shadow that bears a striking resemblance to a\\nblack corner of a GCP, potentially leading to the misclassification observed.\\nRemote Sens. 2024, 16, 794\\n25 of 27\\nFigure 23. Example of the classifier on a full size image. The blue tinted tiles are the ones classified\\nas containing a GCP.\\n6. Conclusions\\nThis study extensively explored various ResNet architectures and their hyperparam-\\neters to accurately localize the center of black and white squared GCP markers within\\nfixed-sized image tiles. Comparisons between deep learning and a purely computer vision\\napproach employing Hough Transform revealed a significant enhancement in performance\\nwith the deep learning method. The ResNet-152 architecture, trained with a batch size of\\n32, a learning rate of 0.0001, and utilizing the Adam optimizer, showcased, to the best of\\nour knowledge, acceptable accuracy with sub-pixel MAE results.\\nNotably, the investigation into automating GCP detection in drone imagery via a\\npurely deep learning approach is a novel contribution in the literature. The study’s out-\\ncomes underline the potential of the proposed pipeline in minimizing human efforts\\nrequired for georeferencing aerial images. However, while promising, more research is\\nessential to further generalize and optimize this approach for real-world applications\\nacross diverse scenarios. Exploring other model architectures and LLMs may contribute to\\nthese goals.\\nAuthor Contributions: Conceptualization, K.P. and D.R.; prototyping, K.P.; methodology, K.P. and\\nS.O.; software, G.M.O.; analysis and validation, G.M.O., K.P., S.O., and D.R.; data curation, G.M.O.\\nand S.O.; writing—original draft, G.M.O.; writing—review and editing, G.M.O., K.P., and S.O.;\\nvisualization, G.M.O.; supervision, K.P., S.O., and D.R.; funding acquisition, D.R. All authors have\\nread and agreed to the published version of the manuscript.\\nFunding: This research received no external funding.\\nData Availability Statement: The datasets presented in this article are not readily available due to\\nrestrictions from 3rd party data providers. Requests to access the datasets should be directed to\\nsam.oswald@vito.be.\\nAcknowledgments: VITO Remote Sensing is acknowledged for providing access to annotated GCP\\ndatasets available on the Mapeo platform. Thanks to Matthew B. Blaschko for their guidance and\\nsupport throughout this project.\\nConflicts of Interest: The authors declare no conflict of interest.\\nRemote Sens. 2024, 16, 794\\n26 of 27\\nReferences\\n1.\\nTiwari, A.; Dixit, A. Unmanned aerial vehicle and geospatial technology pushing the limits of development. Am. J. Eng. Res.\\n2015, 4, 16–21.\\n2.\\nLiba, N.; Berg-Jürgens, J. Accuracy of Orthomosaic Generated by Different Methods in Example of UAV Platform MUST Q. IOP\\nConf. Ser. Mater. Sci. Eng. 2015, 96, 012041. [CrossRef]\\n3.\\nHruska, R.; Mitchell, J.; Anderson, M.; Glenn, N.F. Radiometric and Geometric Analysis of Hyperspectral Imagery Acquired from\\nan Unmanned Aerial Vehicle. Remote Sens. 2012, 4, 2736–2752. [CrossRef]\\n4.\\nLowe, D. Object recognition from local scale-invariant features. In Proceedings of the Seventh IEEE International Conference on\\nComputer Vision, Kerkyra, Greece, 20–27 September 1999; Volume 2, pp. 1150–1157. [CrossRef]\\n5.\\nLowe, D.G. Distinctive image features from scale-invariant keypoints. Int. J. Comput. Vis. 2004, 60, 91–110. [CrossRef]\\n6.\\nZheng, Z.; Xiao, L.; Changkui, S.; Yongzhi, L. UAV tilted images matching research based on POS. Remote. Sens. Land Resour.\\n2016, 28, 87–92. [CrossRef]\\n7.\\nZhang, H.; Wang, L.; Tian, T.; Yin, J. A Review of Unmanned Aerial Vehicle Low-Altitude Remote Sensing (UAV-LARS) Use in\\nAgricultural Monitoring in China. Remote Sens. 2021, 13, 1221. [CrossRef]\\n8.\\nBay, H.; Tuytelaars, T.; Gool, L.V. Surf: Speeded up robust features. In Computer Vision—ECCV 2006: Proceedings of the 9th European\\nConference on Computer Vision, Graz, Austria, 7–13 May 2006; Springer: Berlin/Heidelberg, Germany, 2006; pp. 404–417.\\n9.\\nLandau, H.; Chen, X.; Klose, S.; Leandro, R.; Vollath, U. Trimble’s Rtk and Dgps Solutions in Comparison with Precise Point\\nPositioning. In Observing our Changing Earth; Springer: Berlin/Heidelberg, Germany, 2009; pp. 709–718.\\n10.\\nAlhamlan, S.; Mills, J.; Walker, A.; Saks, T. The influence of ground control points in the triangulation of Leica ADS40 Data. Int.\\nArch. Photogramm. Remote Sens. Spat. Inf. Sci 2004, 35, 495–500.\\n11.\\nLee, H. Ground Control Points Acquisition Using Spot Image-The Operational Comparison. Int. Arch. Photogramm. Remote. Sens.\\n2000, 33, 528–533.\\n12.\\nZhou, G. Determination of Ground Control Points to Subpixel Accuracies for Rectification of Spot Imagery; Indiana State University:\\nTerre Haute, IN, USA, 1990.\\n13.\\nRen, H.; Li, Z.N. Object detection using edge histogram of oriented gradient. In Proceedings of the 2014 IEEE International\\nConference on Image Processing (ICIP), Paris, France, 27–30 October 2014; pp. 4057–4061. [CrossRef]\\n14.\\nJain, A.; Mahajan, M.; Saraf, R. Standardization of the Shape of Ground Control Point (GCP) and the Methodology for Its\\nDetection in Images for UAV-Based Mapping Applications. In Advances in Computer Vision: Proceedings of the 2019 Computer\\nVision Conference (CVC), Las Vegas, NV, USA, 2–3 May 2019; Arai, K., Kapoor, S., Eds.; Springer International Publishing: Cham,\\nSwitzerland, 2020; pp. 459–476.\\n15.\\nYazdani, A.; Aalizadeh, H.; Karimi, F.; Solouki, S.; Soltanian-Zadeh, H. Sub-pixel X-marker detection by Hough transform. In\\nProceedings of the 2018 25th National and 3rd International Iranian Conference on Biomedical Engineering (ICBME), Qom, Iran,\\n29–30 November 2018; pp. 1–6. [CrossRef]\\n16.\\nZhu, Z.; Bao, T.; Hu, Y.; Gong, J. A novel method for fast positioning of non-standardized ground control points in drone images.\\nRemote Sens. 2021, 13, 2849. [CrossRef]\\n17.\\nHarris, C.; Stephens, M. A combined corner and edge detector. Alvey Vis. Conf. 1988, 15, 10–5244.\\n18.\\nBeaudet, P.R. Rotationally Invariant Image Operators. 1978. Available online: https://www.semanticscholar.org/paper/\\nRotationally-invariant-image-operators-Beaudet/b80deba9cce6ad3bc8f5624c4a151a64ee226f14 (accessed on 23 November 2023).\\n19.\\nBarroso-Laguna, A.; Riba, E.; Ponsa, D.; Mikolajczyk, K. Key.Net: Keypoint Detection by Handcrafted and Learned CNN\\nFilters. In Proceedings of the IEEE/CVF International Conference on Computer Vision, Seoul, Republic of Korea, 27 October–2\\nNovember 2019.\\n20.\\nDai, Z.; Huang, X.; Chen, W.; He, L.; Zhang, H. A comparison of CNN-based and hand-crafted keypoint descriptors. In\\nProceedings of the 2019 International Conference on Robotics and Automation (ICRA), Montreal, QC, Canada, 20–24 May 2019;\\npp. 2399–2404.\\n21.\\nHe, K.; Zhang, X.; Ren, S.; Sun, J. Deep Residual Learning for Image Recognition. In Proceedings of the IEEE Conference on\\nComputer Vision and Pattern Recognition, Las Vegas, NV, USA, 27–30 June 2016. [CrossRef]\\n22.\\nHochreiter, S.; Schmidhuber, J. Long Short-term Memory. Neural Comput. 1997, 9, 1735–1780. [CrossRef] [PubMed]\\n23.\\nGrosse, R. Lecture 15: Exploding and Vanishing Gradients; University of Toronto Computer Science: Toronto, ON, Canada, 2017.\\n24.\\nSrivastava, R.K.; Greff, K.; Schmidhuber, J. Training Very Deep Networks. arXiv 2015, arXiv:1507.06228\\n25.\\nSimonyan, K.; Zisserman, A. Very Deep Convolutional Networks for Large-Scale Image Recognition. arXiv 2014, arXiv:1409.1556.\\n26.\\nChollet, F. Xception: Deep Learning with Depthwise Separable Convolutions. In Proceedings of the IEEE Conference on\\nComputer Vision and Pattern Recognition, Honolulu, HI, USA, 21–26 July 2017. [CrossRef]\\n27.\\nSrinivasan, K.; Garg, L.; Datta, D.; Alaboudi, A.A.; Jhanjhi, N.; Agarwal, R.; Thomas, A.G. Performance comparison of deep cnn\\nmodels for detecting driver’s distraction. CMC-Comput. Mater. Contin. 2021, 68, 4109–4124. [CrossRef]\\n28.\\nRyu, B.Y.; Park, W.N.; Jung, D.; Kim, S.-W. Landmark Localization for Drone Aerial Mapping Using GPS and Sparse Point Cloud\\nfor Photogrammetry Pipeline Automation. In Proceedings of the International Conference on Electronics, Information, and\\nCommunication (ICEIC), Jeju, Republic of Korea, 6–9 February 2022.\\n29.\\nBecker, D.; Klonowski, J. Object Recognition of a GCP Design in UAS Imagery Using Deep Learning and Image Processing—Proof\\nof Concept Study. Drones 2023, 7, 94. [CrossRef]\\nRemote Sens. 2024, 16, 794\\n27 of 27\\n30.\\nCheng, C.; Yang, J.; Wang, C.; Zheng, Z.; Li, X.; Dong, D.; Chang, M.; Zhuang, Z. Automatic detection of aerial survey ground\\ncontrol points based on Yolov5-OBB. arXiv 2023, arXiv:2303.03041.\\n31.\\nHe, H.; Qiao, Y.; Li, X.; Chen, C.; Zhang, X. Automatic weight measurement of pigs based on 3D images and regression network.\\nComput. Electron. Agric. 2021, 187, 106299. [CrossRef]\\n32.\\nRyou, S.; Perona, P. Weakly Supervised Keypoint Discovery. arXiv 2021, arXiv:2109.13423.\\n33.\\nWu, S.; Xu, J.; Zhu, S.; Guo, H. A Deep Residual convolutional neural network for facial keypoint detection with missing labels.\\nSignal Process. 2018, 144, 384–391. [CrossRef]\\n34.\\nLin, Y.; Chi, W.; Sun, W.; Liu, S.; Fan, D. Human action recognition algorithm based on improved ResNet and skeletal keypoints\\nin single image. Math. Probl. Eng. 2020, 2020, 6954174. [CrossRef]\\n35.\\nCanny, J. A Computational Approach to Edge Detection. IEEE Trans. Pattern Anal. Mach. Intell. 1986, PAMI-8, 679–698. [CrossRef]\\n36.\\nAggarwal, N.; Karl, W. Line detection in images through regularized hough transform.\\nIEEE Trans. Image Process. 2006,\\n15, 582–591. [CrossRef] [PubMed]\\n37.\\nAquil, M.A.I.; Ishak, W.H.W. Evaluation of scratch and pre-trained convolutional neural networks for the classification of Tomato\\nplant diseases. IAES Int. J. Artif. Intell. 2021, 10, 467.\\n38.\\nDeng, J.; Dong, W.; Socher, R.; Li, L.J.; Li, K.; Li, F.F. ImageNet: A Large-Scale Hierarchical Image Database. In Proceedings of the\\n2009 IEEE Conference on Computer Vision and Pattern Recognition, Miami, FL, USA, 20–25 June 2009; pp. 248–255. [CrossRef]\\n39.\\nGirshick, R. Fast R-CNN. In Proceedings of the IEEE International Conference on Computer Vision, Santiago, Chile, 7–13\\nDecember 2015. [CrossRef]\\n40.\\nKingma, D.P.; Ba, J. Adam: A Method for Stochastic Optimization. arXiv 2014, arXiv:1412.6980.\\n41.\\nRedmon, J.; Divvala, S.K.; Girshick, R.B.; Farhadi, A. You Only Look Once: Unified, Real-Time Object Detection. In Proceedings\\nof the IEEE Conference on Computer Vision and Pattern Recognition, Las Vegas, NV, USA, 27–30 June 2016.\\n42.\\nWang, C.Y.; Bochkovskiy, A.; Liao, H.Y.M. YOLOv7: Trainable bag-of-freebies sets new state-of-the-art for real-time object\\ndetectors. arXiv 2022, arXiv:2207.02696.\\nDisclaimer/Publisher’s Note: The statements, opinions and data contained in all publications are solely those of the individual\\nauthor(s) and contributor(s) and not of MDPI and/or the editor(s). MDPI and/or the editor(s) disclaim responsibility for any injury to\\npeople or property resulting from any ideas, methods, instructions or products referred to in the content.\\n'"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "text:str = ''\n",
    "for page in doc:\n",
    "    text += page.get_text()\n",
    "text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "8c34861a",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_text_splitters import RecursiveCharacterTextSplitter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "d3f6bb33",
   "metadata": {},
   "outputs": [],
   "source": [
    "text_splitter = RecursiveCharacterTextSplitter(\n",
    "    chunk_size=500,\n",
    "    chunk_overlap=5,\n",
    "    length_function=len,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "0a794149",
   "metadata": {},
   "outputs": [],
   "source": [
    "docs = text_splitter.create_documents([text])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "5c54e74d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "161"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(docs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "780f2138",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LLM stands for Large Language Model. It's a type of artificial intelligence (AI) designed to process and generate human-like language. LLMs are trained on vast amounts of text data, which enables them to learn patterns, relationships, and context within language.\n",
      "\n",
      "Large Language Models are typically characterized by their ability to:\n",
      "\n",
      "1. **Understand natural language**: LLMs can comprehend and interpret human language, including nuances, idioms, and context.\n",
      "2. **Generate text**: LLMs can create coherent and contextually relevant text, such as responses to questions, summaries of articles, or even entire articles.\n",
      "3. **Answer questions**: LLMs can provide accurate and informative answers to a wide range of questions, from simple queries to complex, open-ended questions.\n",
      "4. **Translate languages**: LLMs can translate text from one language to another, often with high accuracy.\n",
      "5. **Summarize content**: LLMs can condense long pieces of text into shorter, more digestible summaries.\n",
      "\n",
      "LLMs are often used in various applications, such as:\n",
      "\n",
      "1. **Virtual assistants**: LLMs power virtual assistants like Siri, Google Assistant, and Alexa.\n",
      "2. **Chatbots**: LLMs are used in chatbots to provide customer support, answer frequently asked questions, and engage in conversations.\n",
      "3. **Language translation**: LLMs are used in translation software to translate text and speech in real-time.\n",
      "4. **Content generation**: LLMs are used to generate content, such as articles, social media posts, and product descriptions.\n",
      "5. **Research and analysis**: LLMs are used in research and analysis to summarize large datasets, identify patterns, and provide insights.\n",
      "\n",
      "Some popular examples of LLMs include:\n",
      "\n",
      "1. **BERT (Bidirectional Encoder Representations from Transformers)**: Developed by Google, BERT is a widely used LLM that has achieved state-of-the-art results in various natural language processing tasks.\n",
      "2. **RoBERTa (Robustly Optimized BERT Pretraining Approach)**: Developed by Facebook, RoBERTa is a variant of BERT that has achieved even better results in some tasks.\n",
      "3. **LLaMA (Large Language Model Application)**: Developed by Meta AI, LLaMA is a large language model that has achieved state-of-the-art results in various natural language processing tasks.\n",
      "\n",
      "Overall, LLMs have the potential to revolutionize the way we interact with language and information, and their applications are vast and diverse.\n"
     ]
    }
   ],
   "source": [
    "from langchain_groq import ChatGroq\n",
    "from dotenv import load_dotenv\n",
    "\n",
    "load_dotenv()\n",
    "\n",
    "llm = ChatGroq(\n",
    "    model=\"llama-3.1-8b-instant\",\n",
    "    temperature=0,\n",
    ")\n",
    "\n",
    "ai_msq = llm.invoke(\"what is llm?\")\n",
    "print(ai_msq.content)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "a562fe01",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "AIMessage(content='Ich liebe Programmieren.', additional_kwargs={}, response_metadata={'token_usage': {'completion_tokens': 6, 'prompt_tokens': 50, 'total_tokens': 56, 'completion_time': 0.012578753, 'prompt_time': 0.002903914, 'queue_time': 0.048653728, 'total_time': 0.015482667}, 'model_name': 'llama-3.1-8b-instant', 'system_fingerprint': 'fp_510c177af0', 'finish_reason': 'stop', 'logprobs': None}, id='run--f6271505-eb47-494c-bc76-7cd1538ae0e8-0', usage_metadata={'input_tokens': 50, 'output_tokens': 6, 'total_tokens': 56})"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from langchain_core.prompts import ChatPromptTemplate\n",
    "\n",
    "prompt = ChatPromptTemplate.from_messages(\n",
    "    [\n",
    "        (\n",
    "            \"system\",\n",
    "            \"You are a helpful assistant that translates {input_language} to {output_language}.\",\n",
    "        ),\n",
    "        (\"human\", \"{input}\"),\n",
    "    ]\n",
    ")\n",
    "\n",
    "chain = prompt | llm\n",
    "chain.invoke(\n",
    "    {\n",
    "        \"input_language\": \"English\",\n",
    "        \"output_language\": \"German\",\n",
    "        \"input\": \"I love programming.\",\n",
    "    }\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "f095be43",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_community.vectorstores import FAISS, InMemoryVectorStore"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "cb3c6c92",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_huggingface import HuggingFaceEmbeddings   # LangChain ≥ 0.2\n",
    "# (or: from langchain_community.embeddings import HuggingFaceEmbeddings)\n",
    "\n",
    "emb = HuggingFaceEmbeddings(\n",
    "        model_name=\"sentence-transformers/all-MiniLM-L6-v2\",\n",
    "        # model_kwargs={\"device\": \"cuda\"},\n",
    "        encode_kwargs={\"normalize_embeddings\": True}\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "6593b444",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_community.vectorstores import FAISS\n",
    "from langchain_community.docstore.in_memory import InMemoryDocstore\n",
    "\n",
    "vector_db = FAISS.from_texts(\n",
    "            texts=[doc.page_content for doc in docs],\n",
    "            embedding=emb,          # wrapper above\n",
    "            metadatas=[doc.metadata for doc in docs])    # optional\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e1469337",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
